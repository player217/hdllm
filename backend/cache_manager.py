"""
Cache Manager with Versioning Strategy for HDí˜„ëŒ€ë¯¸í¬ Gauss-1 RAG System
Author: Claude Code
Date: 2025-01-26
Description: Phase 2B-1 - ìºì‹œ í‚¤/ë¬´íš¨í™” ëª…ì„¸ (ë²„ì „ ì „ëµ) êµ¬í˜„
"""

import os
import time
import json
import hashlib
import asyncio
import logging
from typing import Dict, Optional, Any, List, Union, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from collections import OrderedDict
import threading
from contextlib import asynccontextmanager

try:
    import redis.asyncio as redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    redis = None

logger = logging.getLogger(__name__)

class CacheLevel(Enum):
    """ìºì‹œ ë ˆë²¨ ì •ì˜"""
    MEMORY = "memory"        # ë©”ëª¨ë¦¬ ìºì‹œ (ê°€ì¥ ë¹ ë¦„)
    REDIS = "redis"          # Redis ìºì‹œ (ë¶„ì‚° ê°€ëŠ¥)
    PERSISTENT = "persistent" # ë””ìŠ¤í¬ ìºì‹œ (ì˜êµ¬ ë³´ê´€)

class CacheVersion(Enum):
    """ìºì‹œ ë²„ì „ ì „ëµ"""
    CONTENT_HASH = "content_hash"      # ë‚´ìš© ê¸°ë°˜ í•´ì‹œ
    TIMESTAMP = "timestamp"            # ì‹œê°„ ê¸°ë°˜ ë²„ì „
    SEMANTIC = "semantic"              # ì˜ë¯¸ë¡ ì  ë²„ì „ (v1.0.0)
    INCREMENTAL = "incremental"        # ì¦ë¶„ ë²„ì „ (1, 2, 3...)

@dataclass
class CacheConfig:
    """ìºì‹œ ì„¤ì •"""
    # ê¸°ë³¸ ì„¤ì •
    enable_memory_cache: bool = True
    enable_redis_cache: bool = True
    enable_persistent_cache: bool = False
    
    # ë©”ëª¨ë¦¬ ìºì‹œ ì„¤ì •
    memory_max_size: int = int(os.getenv("CACHE_MEMORY_MAX_SIZE", "1000"))
    memory_ttl_seconds: int = int(os.getenv("CACHE_MEMORY_TTL", "3600"))  # 1ì‹œê°„
    
    # Redis ìºì‹œ ì„¤ì •
    redis_url: str = os.getenv("CACHE_REDIS_URL", "redis://localhost:6379")
    redis_ttl_seconds: int = int(os.getenv("CACHE_REDIS_TTL", "86400"))    # 24ì‹œê°„
    redis_key_prefix: str = os.getenv("CACHE_REDIS_PREFIX", "hdllm:cache:")
    
    # ë²„ì „ ì „ëµ
    default_version_strategy: CacheVersion = CacheVersion.CONTENT_HASH
    version_separator: str = "#v#"
    
    # ë¬´íš¨í™” ì„¤ì •
    auto_cleanup_interval: int = int(os.getenv("CACHE_CLEANUP_INTERVAL", "300"))  # 5ë¶„
    batch_invalidation_size: int = int(os.getenv("CACHE_BATCH_INVALIDATION", "100"))

@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬"""
    key: str
    value: Any
    version: str
    created_at: float
    accessed_at: float
    ttl: int
    level: CacheLevel
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    @property
    def is_expired(self) -> bool:
        """ë§Œë£Œ ì—¬ë¶€ í™•ì¸"""
        if self.ttl <= 0:  # TTLì´ 0ì´ë©´ ë¬´ì œí•œ
            return False
        return time.time() - self.created_at > self.ttl
    
    @property
    def age_seconds(self) -> float:
        """ìºì‹œ ë‚˜ì´ (ì´ˆ)"""
        return time.time() - self.created_at
    
    def touch(self):
        """ì ‘ê·¼ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        self.accessed_at = time.time()

class VersionedCacheKey:
    """ë²„ì „ ê¸°ë°˜ ìºì‹œ í‚¤ ê´€ë¦¬"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        
    def generate_key(self, namespace: str, identifier: str, 
                    data: Any = None, strategy: CacheVersion = None) -> str:
        """ë²„ì „ ê¸°ë°˜ í‚¤ ìƒì„±"""
        strategy = strategy or self.config.default_version_strategy
        version = self._generate_version(data, strategy)
        
        # í‚¤ í˜•ì‹: namespace:identifier#v#version
        base_key = f"{namespace}:{identifier}"
        versioned_key = f"{base_key}{self.config.version_separator}{version}"
        
        return versioned_key
    
    def parse_key(self, key: str) -> Tuple[str, str]:
        """í‚¤ì—ì„œ ê¸°ë³¸í‚¤ì™€ ë²„ì „ ë¶„ë¦¬"""
        if self.config.version_separator in key:
            base_key, version = key.rsplit(self.config.version_separator, 1)
            return base_key, version
        return key, ""
    
    def _generate_version(self, data: Any, strategy: CacheVersion) -> str:
        """ë²„ì „ ìƒì„± ì „ëµë³„ êµ¬í˜„"""
        if strategy == CacheVersion.CONTENT_HASH:
            if data is not None:
                content_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
                hash_result = hashlib.md5(content_str.encode()).hexdigest()[:8]
                logger.debug(f"Content hash: '{content_str}' -> {hash_result}")
                return hash_result
            time_hash = hashlib.md5(str(time.time()).encode()).hexdigest()[:8]
            logger.debug(f"Time-based hash: {time_hash}")
            return time_hash
        
        elif strategy == CacheVersion.TIMESTAMP:
            return str(int(time.time()))
        
        elif strategy == CacheVersion.SEMANTIC:
            # ê¸°ë³¸ semantic version (ì‹¤ì œë¡œëŠ” ì™¸ë¶€ì—ì„œ ì£¼ì…ë°›ì•„ì•¼ í•¨)
            return "1.0.0"
        
        elif strategy == CacheVersion.INCREMENTAL:
            # ì¦ë¶„ ë²„ì „ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ì¹´ìš´í„° ì‚¬ìš©)
            return str(int(time.time()) % 1000000)  # ê°„ë‹¨í•œ êµ¬í˜„
        
        return "default"

class MemoryCache:
    """ë©”ëª¨ë¦¬ ê¸°ë°˜ LRU ìºì‹œ"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.lock = threading.RLock()
        self.stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "sets": 0
        }
    
    def get(self, key: str) -> Optional[CacheEntry]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        with self.lock:
            if key in self.cache:
                entry = self.cache[key]
                if entry.is_expired:
                    del self.cache[key]
                    self.stats["misses"] += 1
                    return None
                
                # LRU ì—…ë°ì´íŠ¸
                self.cache.move_to_end(key)
                entry.touch()
                self.stats["hits"] += 1
                return entry
            
            self.stats["misses"] += 1
            return None
    
    def set(self, key: str, value: Any, version: str, ttl: int = None) -> bool:
        """ìºì‹œì— ê°’ ì €ì¥"""
        with self.lock:
            ttl = ttl or self.config.memory_ttl_seconds
            
            entry = CacheEntry(
                key=key,
                value=value,
                version=version,
                created_at=time.time(),
                accessed_at=time.time(),
                ttl=ttl,
                level=CacheLevel.MEMORY
            )
            
            # ê¸°ì¡´ í•­ëª© ì œê±° ë° ìƒˆ í•­ëª© ì¶”ê°€
            if key in self.cache:
                del self.cache[key]
            
            self.cache[key] = entry
            self.stats["sets"] += 1
            
            # í¬ê¸° ì œí•œ í™•ì¸ ë° LRU ì œê±°
            self._evict_if_needed()
            
            return True
    
    def delete(self, key: str) -> bool:
        """ìºì‹œì—ì„œ í•­ëª© ì‚­ì œ"""
        with self.lock:
            if key in self.cache:
                del self.cache[key]
                return True
            return False
    
    def delete_by_pattern(self, pattern: str) -> int:
        """íŒ¨í„´ìœ¼ë¡œ í•­ëª©ë“¤ ì‚­ì œ"""
        import re
        deleted_count = 0
        
        with self.lock:
            keys_to_delete = []
            for key in self.cache.keys():
                if re.match(pattern, key):
                    keys_to_delete.append(key)
            
            for key in keys_to_delete:
                del self.cache[key]
                deleted_count += 1
        
        return deleted_count
    
    def clear(self) -> int:
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        with self.lock:
            count = len(self.cache)
            self.cache.clear()
            return count
    
    def cleanup_expired(self) -> int:
        """ë§Œë£Œëœ í•­ëª© ì •ë¦¬"""
        expired_keys = []
        
        with self.lock:
            for key, entry in self.cache.items():
                if entry.is_expired:
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self.cache[key]
        
        return len(expired_keys)
    
    def _evict_if_needed(self):
        """í•„ìš”ì‹œ LRU ì œê±°"""
        while len(self.cache) > self.config.memory_max_size:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
            self.stats["evictions"] += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        with self.lock:
            total_requests = self.stats["hits"] + self.stats["misses"]
            hit_rate = self.stats["hits"] / total_requests if total_requests > 0 else 0
            
            return {
                "level": "memory",
                "size": len(self.cache),
                "max_size": self.config.memory_max_size,
                "hit_rate": hit_rate,
                **self.stats
            }

class RedisCache:
    """Redis ê¸°ë°˜ ë¶„ì‚° ìºì‹œ"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.redis_client: Optional[redis.Redis] = None
        self.stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "errors": 0
        }
    
    async def initialize(self):
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        if not REDIS_AVAILABLE:
            logger.warning("ğŸ”´ Redis not available, skipping Redis cache")
            return
        
        try:
            self.redis_client = redis.from_url(
                self.config.redis_url,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
                retry_on_timeout=True
            )
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            await self.redis_client.ping()
            logger.info("âœ… Redis cache initialized")
            
        except Exception as e:
            logger.error(f"âŒ Redis initialization failed: {e}")
            self.redis_client = None
    
    async def get(self, key: str) -> Optional[CacheEntry]:
        """Redisì—ì„œ ê°’ ì¡°íšŒ"""
        if not self.redis_client:
            return None
        
        try:
            redis_key = f"{self.config.redis_key_prefix}{key}"
            raw_data = await self.redis_client.get(redis_key)
            
            if raw_data:
                entry_data = json.loads(raw_data)
                entry = CacheEntry(
                    key=entry_data["key"],
                    value=entry_data["value"],
                    version=entry_data["version"],
                    created_at=entry_data["created_at"],
                    accessed_at=time.time(),  # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                    ttl=entry_data["ttl"],
                    level=CacheLevel.REDIS,
                    metadata=entry_data.get("metadata", {})
                )
                
                self.stats["hits"] += 1
                return entry
            
            self.stats["misses"] += 1
            return None
            
        except Exception as e:
            logger.error(f"Redis get error: {e}")
            self.stats["errors"] += 1
            return None
    
    async def set(self, key: str, value: Any, version: str, ttl: int = None) -> bool:
        """Redisì— ê°’ ì €ì¥"""
        if not self.redis_client:
            return False
        
        try:
            ttl = ttl or self.config.redis_ttl_seconds
            redis_key = f"{self.config.redis_key_prefix}{key}"
            
            entry_data = {
                "key": key,
                "value": value,
                "version": version,
                "created_at": time.time(),
                "accessed_at": time.time(),
                "ttl": ttl,
                "metadata": {}
            }
            
            await self.redis_client.setex(
                redis_key,
                ttl,
                json.dumps(entry_data, ensure_ascii=False)
            )
            
            self.stats["sets"] += 1
            return True
            
        except Exception as e:
            logger.error(f"Redis set error: {e}")
            self.stats["errors"] += 1
            return False
    
    async def delete(self, key: str) -> bool:
        """Redisì—ì„œ í•­ëª© ì‚­ì œ"""
        if not self.redis_client:
            return False
        
        try:
            redis_key = f"{self.config.redis_key_prefix}{key}"
            result = await self.redis_client.delete(redis_key)
            return result > 0
            
        except Exception as e:
            logger.error(f"Redis delete error: {e}")
            self.stats["errors"] += 1
            return False
    
    async def delete_by_pattern(self, pattern: str) -> int:
        """íŒ¨í„´ìœ¼ë¡œ Redis í•­ëª©ë“¤ ì‚­ì œ"""
        if not self.redis_client:
            return 0
        
        try:
            redis_pattern = f"{self.config.redis_key_prefix}{pattern}"
            keys = await self.redis_client.keys(redis_pattern)
            
            if keys:
                deleted = await self.redis_client.delete(*keys)
                return deleted
            
            return 0
            
        except Exception as e:
            logger.error(f"Redis delete by pattern error: {e}")
            self.stats["errors"] += 1
            return 0
    
    async def clear(self) -> int:
        """ëª¨ë“  ìºì‹œ ì‚­ì œ (prefix ê¸°ì¤€)"""
        if not self.redis_client:
            return 0
        
        try:
            pattern = f"{self.config.redis_key_prefix}*"
            keys = await self.redis_client.keys(pattern)
            
            if keys:
                deleted = await self.redis_client.delete(*keys)
                return deleted
            
            return 0
            
        except Exception as e:
            logger.error(f"Redis clear error: {e}")
            self.stats["errors"] += 1
            return 0
    
    async def cleanup(self):
        """Redis ì—°ê²° ì¢…ë£Œ"""
        if self.redis_client:
            await self.redis_client.close()
    
    def get_stats(self) -> Dict[str, Any]:
        """Redis ìºì‹œ í†µê³„"""
        total_requests = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / total_requests if total_requests > 0 else 0
        
        return {
            "level": "redis",
            "connected": self.redis_client is not None,
            "hit_rate": hit_rate,
            **self.stats
        }

class MultiLevelCacheManager:
    """ë‹¤ì¸µ ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self, config: CacheConfig = None):
        self.config = config or CacheConfig()
        self.key_manager = VersionedCacheKey(self.config)
        
        # ìºì‹œ ë ˆë²¨ ì´ˆê¸°í™”
        self.memory_cache = MemoryCache(self.config) if self.config.enable_memory_cache else None
        self.redis_cache = RedisCache(self.config) if self.config.enable_redis_cache else None
        
        # ì •ë¦¬ ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬
        self._cleanup_task = None
        self._running = False
    
    async def initialize(self):
        """ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        logger.info("ğŸš€ Initializing Multi-Level Cache Manager")
        
        if self.redis_cache:
            await self.redis_cache.initialize()
        
        # ìë™ ì •ë¦¬ ì‘ì—… ì‹œì‘
        if self.config.auto_cleanup_interval > 0:
            self._running = True
            self._cleanup_task = asyncio.create_task(self._auto_cleanup())
        
        logger.info("âœ… Multi-Level Cache Manager initialized")
    
    async def get(self, namespace: str, identifier: str, 
                 version_data: Any = None, strategy: CacheVersion = None) -> Optional[Any]:
        """ë‹¤ì¸µ ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        # version_dataê°€ Noneì´ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ì‹ë³„ì ê¸°ë°˜ í‚¤ ìƒì„±ì„ ì‹œë„í•˜ì§€ë§Œ,
        # ì´ëŠ” setê³¼ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ Noneìœ¼ë¡œ ìœ ì§€í•˜ì—¬ ì¼ê´€ì„± í™•ë³´
        key = self.key_manager.generate_key(namespace, identifier, version_data, strategy)
        
        # L1: Memory Cache
        if self.memory_cache:
            entry = self.memory_cache.get(key)
            if entry:
                logger.debug(f"Cache HIT (Memory): {key}")
                return entry.value
        
        # L2: Redis Cache
        if self.redis_cache:
            entry = await self.redis_cache.get(key)
            if entry:
                logger.debug(f"Cache HIT (Redis): {key}")
                
                # Memoryì— ìŠ¹ê²©
                if self.memory_cache:
                    self.memory_cache.set(key, entry.value, entry.version)
                
                return entry.value
        
        logger.debug(f"Cache MISS: {key}")
        return None
    
    async def set(self, namespace: str, identifier: str, value: Any,
                 version_data: Any = None, strategy: CacheVersion = None, 
                 ttl: int = None) -> str:
        """ë‹¤ì¸µ ìºì‹œì— ê°’ ì €ì¥"""
        # version_dataê°€ Noneì´ë©´ valueë¥¼ ì‚¬ìš© (ê¸°ë³¸ ë™ì‘)
        if version_data is None:
            version_data = value
        key = self.key_manager.generate_key(namespace, identifier, version_data, strategy)
        _, version = self.key_manager.parse_key(key)
        
        # ëª¨ë“  ë ˆë²¨ì— ì €ì¥
        success_levels = []
        
        if self.memory_cache:
            if self.memory_cache.set(key, value, version, ttl):
                success_levels.append("memory")
        
        if self.redis_cache:
            if await self.redis_cache.set(key, value, version, ttl):
                success_levels.append("redis")
        
        logger.debug(f"Cache SET ({','.join(success_levels)}): {key}")
        return key
    
    async def delete(self, namespace: str, identifier: str) -> int:
        """íŠ¹ì • í‚¤ì˜ ëª¨ë“  ë²„ì „ ì‚­ì œ"""
        base_pattern = f"{namespace}:{identifier}"
        deleted_count = 0
        
        # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ëª¨ë“  ë²„ì „ ì‚­ì œ
        if self.memory_cache:
            deleted_count += self.memory_cache.delete_by_pattern(f"{base_pattern}.*")
        
        if self.redis_cache:
            deleted_count += await self.redis_cache.delete_by_pattern(f"{base_pattern}*")
        
        logger.info(f"Cache DELETE: {base_pattern} ({deleted_count} entries)")
        return deleted_count
    
    async def invalidate_by_version(self, namespace: str, old_version: str) -> int:
        """íŠ¹ì • ë²„ì „ ì´í•˜ì˜ ìºì‹œ ë¬´íš¨í™”"""
        pattern = f"{namespace}:.*{self.config.version_separator}{old_version}"
        deleted_count = 0
        
        if self.memory_cache:
            deleted_count += self.memory_cache.delete_by_pattern(pattern)
        
        if self.redis_cache:
            deleted_count += await self.redis_cache.delete_by_pattern(pattern.replace(".*", "*"))
        
        logger.info(f"Cache INVALIDATE by version: {namespace} <= {old_version} ({deleted_count} entries)")
        return deleted_count
    
    async def invalidate_namespace(self, namespace: str) -> int:
        """ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì „ì²´ ë¬´íš¨í™”"""
        pattern = f"{namespace}:*"
        deleted_count = 0
        
        if self.memory_cache:
            deleted_count += self.memory_cache.delete_by_pattern(f"{namespace}:.*")
        
        if self.redis_cache:
            deleted_count += await self.redis_cache.delete_by_pattern(pattern)
        
        logger.info(f"Cache INVALIDATE namespace: {namespace} ({deleted_count} entries)")
        return deleted_count
    
    async def clear_all(self) -> Dict[str, int]:
        """ëª¨ë“  ìºì‹œ ì‚­ì œ"""
        cleared = {}
        
        if self.memory_cache:
            cleared["memory"] = self.memory_cache.clear()
        
        if self.redis_cache:
            cleared["redis"] = await self.redis_cache.clear()
        
        logger.info(f"Cache CLEAR ALL: {cleared}")
        return cleared
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        stats = {
            "timestamp": datetime.now().isoformat(),
            "levels": {}
        }
        
        if self.memory_cache:
            stats["levels"]["memory"] = self.memory_cache.get_stats()
        
        if self.redis_cache:
            stats["levels"]["redis"] = self.redis_cache.get_stats()
        
        return stats
    
    async def _auto_cleanup(self):
        """ìë™ ì •ë¦¬ ì‘ì—…"""
        while self._running:
            try:
                if self.memory_cache:
                    expired_count = self.memory_cache.cleanup_expired()
                    if expired_count > 0:
                        logger.debug(f"ğŸ§¹ Memory cache cleanup: {expired_count} expired entries")
                
                await asyncio.sleep(self.config.auto_cleanup_interval)
                
            except Exception as e:
                logger.error(f"Cache cleanup error: {e}")
                await asyncio.sleep(60)  # 1ë¶„ ëŒ€ê¸° í›„ ì¬ì‹œë„
    
    async def cleanup(self):
        """ìºì‹œ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        logger.info("ğŸ§¹ Cleaning up Multi-Level Cache Manager")
        
        self._running = False
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        if self.redis_cache:
            await self.redis_cache.cleanup()
        
        logger.info("âœ… Multi-Level Cache Manager cleanup complete")

# ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_cache_manager: Optional[MultiLevelCacheManager] = None

async def get_cache_manager() -> MultiLevelCacheManager:
    """ê¸€ë¡œë²Œ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _cache_manager
    
    if _cache_manager is None:
        config = CacheConfig()
        _cache_manager = MultiLevelCacheManager(config)
        await _cache_manager.initialize()
    
    return _cache_manager

@asynccontextmanager
async def cache_context():
    """ìºì‹œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    cache_manager = await get_cache_manager()
    try:
        yield cache_manager
    finally:
        # ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œì‹œ ì •ë¦¬ëŠ” í•˜ì§€ ì•ŠìŒ (ê¸€ë¡œë²Œ ì¸ìŠ¤í„´ìŠ¤)
        pass