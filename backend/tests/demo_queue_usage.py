"""
í ì‹œìŠ¤í…œ ì‚¬ìš©ë²• ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸
Author: Claude Code
Date: 2025-01-27
Description: ìƒˆë¡œìš´ TaskQueue ê¸°ë°˜ ì‹œìŠ¤í…œ ì‚¬ìš©ë²• ì‹œì—°
"""

import asyncio
import time
import logging
import sys
import os
from pathlib import Path

# ìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))

from backend.resource_manager import ResourceManager
from backend.pipeline.async_pipeline import AsyncPipeline
from backend.pipeline.task_queue import TaskQueue, QueueTask, TaskStatus

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s'
)
logger = logging.getLogger(__name__)

class QueueSystemDemo:
    """í ì‹œìŠ¤í…œ ì‚¬ìš©ë²• ë°ëª¨"""
    
    def __init__(self):
        self.resource_manager = None
        self.pipeline = None
    
    async def setup(self):
        """ë°ëª¨ í™˜ê²½ ì„¤ì •"""
        logger.info("ğŸ”§ Setting up demo environment...")
        
        # ResourceManager ì´ˆê¸°í™”
        self.resource_manager = ResourceManager()
        await self.resource_manager.initialize()
        
        # AsyncPipeline ì´ˆê¸°í™” (ì‘ì€ í ì‚¬ì´ì¦ˆë¡œ ë°ëª¨)
        self.pipeline = AsyncPipeline(
            resource_manager=self.resource_manager,
            max_concurrent=2,  # ì‘ì—…ì 2ëª…
            max_queue_size=10  # í í¬ê¸° 10
        )
        
        # í ì‹œìŠ¤í…œ ì‹œì‘
        await self.pipeline.start_queue()
        
        logger.info("âœ… Demo environment ready!")
    
    async def cleanup(self):
        """í™˜ê²½ ì •ë¦¬"""
        logger.info("ğŸ§¹ Cleaning up...")
        
        if self.pipeline:
            await self.pipeline.stop_queue()
        
        if self.resource_manager:
            await self.resource_manager.cleanup()
        
        logger.info("âœ… Cleanup complete!")
    
    async def demo_basic_queueing(self):
        """ê¸°ë³¸ íì‰ ë°ëª¨"""
        logger.info("\n" + "="*50)
        logger.info("ğŸ“¤ DEMO 1: Basic Task Queueing")
        logger.info("="*50)
        
        # 1. ê²€ìƒ‰ ì‘ì—…ë“¤ì„ íì— ì¶”ê°€
        search_queries = [
            "GPU ê°€ì†ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©ë²•",
            "ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ìµœì í™”",
            "ë¹„ë™ê¸° ì²˜ë¦¬ ì¥ì "
        ]
        
        logger.info(f"ğŸ“ Adding {len(search_queries)} search tasks to queue...")
        
        task_ids = []
        for i, query in enumerate(search_queries):
            payload = {
                "query": query,
                "source_type": "mail",
                "limit": 3,
                "score_threshold": 0.3
            }
            
            # ìš°ì„ ìˆœìœ„ ì„¤ì • (ë†’ì€ ìˆ«ì = ë†’ì€ ìš°ì„ ìˆœìœ„)
            priority = len(search_queries) - i  # ì²« ë²ˆì§¸ê°€ ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„
            
            task_id = await self.pipeline.enqueue("search", payload, priority=priority)
            task_ids.append(task_id)
            
            logger.info(f"  ğŸ“¤ Task {i+1}: {task_id} (priority: {priority})")
        
        # 2. í ìƒíƒœ ëª¨ë‹ˆí„°ë§
        logger.info("\nğŸ“Š Monitoring queue progress...")
        
        for _ in range(15):  # ìµœëŒ€ 30ì´ˆ ëª¨ë‹ˆí„°ë§
            stats = self.pipeline.get_queue_stats()
            logger.info(f"  Queue: {stats.get('pending_tasks', 0)} pending, "
                       f"{stats.get('processing_tasks', 0)} processing, "
                       f"{stats.get('total_completed', 0)} completed")
            
            if stats.get('pending_tasks', 0) == 0 and stats.get('processing_tasks', 0) == 0:
                break
            
            await asyncio.sleep(2)
        
        logger.info("âœ… Basic queueing demo completed!")
        return task_ids
    
    async def demo_priority_queueing(self):
        """ìš°ì„ ìˆœìœ„ íì‰ ë°ëª¨"""
        logger.info("\n" + "="*50)
        logger.info("âš¡ DEMO 2: Priority-Based Queueing")
        logger.info("="*50)
        
        # ë‹¤ì–‘í•œ ìš°ì„ ìˆœìœ„ì˜ ì‘ì—…ë“¤
        tasks_with_priority = [
            {"type": "search", "query": "ê¸´ê¸‰ ì§ˆë¬¸", "priority": 10},
            {"type": "embed", "text": "ì¼ë°˜ ì„ë² ë”©", "priority": 1},
            {"type": "search", "query": "ì¤‘ìš”í•œ ì§ˆë¬¸", "priority": 5},
            {"type": "embed", "text": "ê¸´ê¸‰ ì„ë² ë”©", "priority": 8},
            {"type": "search", "query": "ì¼ë°˜ ì§ˆë¬¸", "priority": 1}
        ]
        
        logger.info("ğŸ“ Adding tasks with different priorities...")
        
        for i, task in enumerate(tasks_with_priority):
            if task["type"] == "search":
                payload = {
                    "query": task["query"],
                    "source_type": "mail",
                    "limit": 2
                }
            else:  # embed
                payload = {
                    "text": task["text"],
                    "document_id": f"demo_doc_{i}",
                    "chunk_idx": 0
                }
            
            task_id = await self.pipeline.enqueue(task["type"], payload, priority=task["priority"])
            logger.info(f"  ğŸ“¤ {task['type']}: {task_id} (priority: {task['priority']})")
            
            # ì‘ì€ ì§€ì—°ìœ¼ë¡œ íì— ìˆœì„œëŒ€ë¡œ ë“¤ì–´ê°€ë„ë¡
            await asyncio.sleep(0.1)
        
        # í ì²˜ë¦¬ ëª¨ë‹ˆí„°ë§
        logger.info("\nğŸ“Š Priority processing order:")
        
        start_time = time.time()
        while time.time() - start_time < 30:  # 30ì´ˆ ì œí•œ
            stats = self.pipeline.get_queue_stats()
            
            if stats.get('pending_tasks', 0) == 0 and stats.get('processing_tasks', 0) == 0:
                break
            
            await asyncio.sleep(2)
        
        logger.info("âœ… Priority queueing demo completed!")
    
    async def demo_batch_operations(self):
        """ë°°ì¹˜ ì‘ì—… ë°ëª¨"""
        logger.info("\n" + "="*50)
        logger.info("ğŸ“¦ DEMO 3: Batch Operations")
        logger.info("="*50)
        
        # ë°°ì¹˜ ì‘ì—… ì •ì˜
        batch_tasks = [
            {
                "task_type": "embed",
                "payload": {
                    "text": f"ë°°ì¹˜ í…ìŠ¤íŠ¸ {i+1}",
                    "document_id": f"batch_doc_{i+1}",
                    "chunk_idx": i
                },
                "priority": 2
            }
            for i in range(5)
        ]
        
        # ê²€ìƒ‰ ì‘ì—…ë“¤ë„ ì¶”ê°€
        batch_tasks.extend([
            {
                "task_type": "search",
                "payload": {
                    "query": f"ë°°ì¹˜ ê²€ìƒ‰ {i+1}",
                    "source_type": "mail",
                    "limit": 2
                },
                "priority": 3
            }
            for i in range(3)
        ])
        
        logger.info(f"ğŸ“ Adding batch of {len(batch_tasks)} tasks...")
        
        # ë°°ì¹˜ë¡œ íì— ì¶”ê°€
        task_ids = await self.pipeline.enqueue_batch(batch_tasks, default_priority=1)
        
        successful_tasks = [tid for tid in task_ids if tid is not None]
        logger.info(f"  ğŸ“¤ Successfully enqueued: {len(successful_tasks)}/{len(batch_tasks)} tasks")
        
        # ì²˜ë¦¬ ì§„í–‰ ëª¨ë‹ˆí„°ë§
        logger.info("\nğŸ“Š Batch processing progress:")
        
        start_time = time.time()
        last_completed = 0
        
        while time.time() - start_time < 45:  # 45ì´ˆ ì œí•œ
            stats = self.pipeline.get_queue_stats()
            completed = stats.get('total_completed', 0)
            
            if completed > last_completed:
                logger.info(f"  Progress: {completed} tasks completed")
                last_completed = completed
            
            if stats.get('pending_tasks', 0) == 0 and stats.get('processing_tasks', 0) == 0:
                break
            
            await asyncio.sleep(3)
        
        logger.info("âœ… Batch operations demo completed!")
    
    async def demo_task_monitoring(self):
        """ì‘ì—… ëª¨ë‹ˆí„°ë§ ë°ëª¨"""
        logger.info("\n" + "="*50)
        logger.info("ğŸ‘€ DEMO 4: Task Monitoring & Management")
        logger.info("="*50)
        
        # ì¥ì‹œê°„ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ ê²€ìƒ‰ ì‘ì—…
        task_id = await self.pipeline.enqueue("search", {
            "query": "ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸",
            "source_type": "mail",
            "limit": 5
        }, priority=1)
        
        logger.info(f"ğŸ“¤ Monitoring task: {task_id}")
        
        # ì‘ì—… ìƒíƒœ ì¶”ì 
        for i in range(10):
            task_status = await self.pipeline.get_task_status(task_id)
            
            if task_status:
                logger.info(f"  Status: {task_status.status.value}, "
                           f"Duration: {task_status.duration or 0:.3f}s")
                
                if task_status.status in [TaskStatus.COMPLETED, TaskStatus.FAILED]:
                    break
            else:
                logger.info("  Task not found in queue (may be completed)")
                break
            
            await asyncio.sleep(2)
        
        # ì „ì²´ í í†µê³„
        logger.info("\nğŸ“Š Final Queue Statistics:")
        stats = self.pipeline.get_stats()
        
        for key, value in stats.items():
            if isinstance(value, dict):
                logger.info(f"  {key}:")
                for sub_key, sub_value in value.items():
                    logger.info(f"    {sub_key}: {sub_value}")
            else:
                logger.info(f"  {key}: {value}")
        
        logger.info("âœ… Task monitoring demo completed!")
    
    async def demo_error_scenarios(self):
        """ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ ë°ëª¨"""
        logger.info("\n" + "="*50)
        logger.info("âš ï¸ DEMO 5: Error Handling")
        logger.info("="*50)
        
        # 1. ì˜ëª»ëœ ì‘ì—… íƒ€ì…
        logger.info("ğŸ§ª Testing invalid task type...")
        try:
            await self.pipeline.enqueue("invalid_task", {"data": "test"})
            logger.info("  âŒ Should have failed!")
        except Exception as e:
            logger.info(f"  âœ… Properly rejected: {e}")
        
        # 2. ë¹ˆ í˜ì´ë¡œë“œë¡œ ê²€ìƒ‰
        logger.info("\nğŸ§ª Testing empty search payload...")
        try:
            await self.pipeline.enqueue("search", {})
            logger.info("  âš ï¸ Enqueued (will fail during processing)")
        except Exception as e:
            logger.info(f"  âœ… Rejected at enqueue: {e}")
        
        # 3. DLQ ìƒíƒœ í™•ì¸
        await asyncio.sleep(5)  # ì²˜ë¦¬ ëŒ€ê¸°
        
        dlq_stats = self.pipeline.dlq.get_stats()
        logger.info(f"\nğŸ“Š Dead Letter Queue Stats: {dlq_stats}")
        
        logger.info("âœ… Error handling demo completed!")
    
    async def run_all_demos(self):
        """ëª¨ë“  ë°ëª¨ ì‹¤í–‰"""
        logger.info("ğŸ¬ Starting Queue System Demo")
        logger.info("This demo shows how to use the new TaskQueue-based system")
        
        await self.setup()
        
        try:
            # ê° ë°ëª¨ ì‹¤í–‰
            await self.demo_basic_queueing()
            await asyncio.sleep(2)
            
            await self.demo_priority_queueing()
            await asyncio.sleep(2)
            
            await self.demo_batch_operations()
            await asyncio.sleep(2)
            
            await self.demo_task_monitoring()
            await asyncio.sleep(2)
            
            await self.demo_error_scenarios()
            
            logger.info("\n" + "="*60)
            logger.info("ğŸ‰ All demos completed successfully!")
            logger.info("="*60)
            
        finally:
            await self.cleanup()


async def main():
    """ë©”ì¸ ë°ëª¨ í•¨ìˆ˜"""
    demo = QueueSystemDemo()
    await demo.run_all_demos()


if __name__ == "__main__":
    asyncio.run(main())