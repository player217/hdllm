"""
RAG-Specific Cache Layer for HDí˜„ëŒ€ë¯¸í¬ Gauss-1 RAG System
Author: Claude Code  
Date: 2025-01-26
Description: Phase 2B-1 - RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ì „ìš© ìºì‹œ ë ˆì´ì–´ êµ¬í˜„
"""

import json
import hashlib
import logging
import asyncio
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
from cache_manager import (
    MultiLevelCacheManager, CacheConfig, CacheVersion, 
    get_cache_manager
)

logger = logging.getLogger(__name__)

@dataclass
class EmbeddingCacheEntry:
    """ì„ë² ë”© ìºì‹œ ì—”íŠ¸ë¦¬"""
    query: str
    embedding: List[float]
    model_name: str
    model_version: str
    created_at: float
    
@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼"""
    query: str
    documents: List[Dict[str, Any]]
    embeddings: List[float]
    similarity_scores: List[float]
    search_params: Dict[str, Any]
    cached: bool = False

@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ"""
    prompt: str
    response: str
    model_name: str
    parameters: Dict[str, Any]
    tokens_used: int
    cached: bool = False

class RAGCacheNamespaces:
    """RAG ìºì‹œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì •ì˜"""
    EMBEDDINGS = "rag:embeddings"
    VECTOR_SEARCH = "rag:vector_search"
    LLM_RESPONSES = "rag:llm_responses"
    DOCUMENT_METADATA = "rag:doc_metadata"
    USER_SESSIONS = "rag:user_sessions"
    SYSTEM_CONFIG = "rag:system_config"

class RAGCacheManager:
    """RAG ì „ìš© ìºì‹œ ê´€ë¦¬ì"""
    
    def __init__(self):
        self.cache_manager: Optional[MultiLevelCacheManager] = None
        self.embedding_model_version = "bge-m3-v1.0"  # ì„ë² ë”© ëª¨ë¸ ë²„ì „
    
    async def initialize(self):
        """RAG ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.cache_manager = await get_cache_manager()
        logger.info("ğŸ¯ RAG Cache Manager initialized")
    
    # ============================================================================
    # ì„ë² ë”© ìºì‹œ ê´€ë¦¬
    # ============================================================================
    
    async def get_embedding(self, query: str, model_name: str = None) -> Optional[List[float]]:
        """ì„ë² ë”© ìºì‹œì—ì„œ ì¡°íšŒ"""
        model_name = model_name or self.embedding_model_version
        
        # ì¿¼ë¦¬ ì •ê·œí™”
        normalized_query = self._normalize_query(query)
        cache_key = f"{normalized_query}:{model_name}"
        
        cached_entry = await self.cache_manager.get(
            RAGCacheNamespaces.EMBEDDINGS,
            cache_key,
            version_data={"query": normalized_query, "model": model_name},
            strategy=CacheVersion.CONTENT_HASH
        )
        
        if cached_entry and isinstance(cached_entry, dict):
            logger.debug(f"ğŸ¯ Embedding cache HIT: {normalized_query[:50]}...")
            return cached_entry.get("embedding")
        
        logger.debug(f"âŒ Embedding cache MISS: {normalized_query[:50]}...")
        return None
    
    async def set_embedding(self, query: str, embedding: List[float], 
                           model_name: str = None, ttl: int = 3600) -> str:
        """ì„ë² ë”© ìºì‹œì— ì €ì¥"""
        model_name = model_name or self.embedding_model_version
        
        normalized_query = self._normalize_query(query)
        cache_key = f"{normalized_query}:{model_name}"
        
        cache_entry = {
            "query": normalized_query,
            "embedding": embedding,
            "model_name": model_name,
            "model_version": self.embedding_model_version,
            "created_at": datetime.now().timestamp(),
            "embedding_dim": len(embedding)
        }
        
        cached_key = await self.cache_manager.set(
            RAGCacheNamespaces.EMBEDDINGS,
            cache_key,
            cache_entry,
            version_data={"query": normalized_query, "model": model_name},
            strategy=CacheVersion.CONTENT_HASH,
            ttl=ttl
        )
        
        logger.debug(f"ğŸ’¾ Embedding cached: {normalized_query[:50]}...")
        return cached_key
    
    # ============================================================================
    # ë²¡í„° ê²€ìƒ‰ ìºì‹œ ê´€ë¦¬  
    # ============================================================================
    
    async def get_vector_search_result(self, query: str, search_params: Dict[str, Any], 
                                     source: str = "mail") -> Optional[SearchResult]:
        """ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì¡°íšŒ"""
        cache_key = self._generate_search_cache_key(query, search_params, source)
        
        cached_result = await self.cache_manager.get(
            RAGCacheNamespaces.VECTOR_SEARCH,
            cache_key,
            version_data={"query": query, "params": search_params},
            strategy=CacheVersion.CONTENT_HASH
        )
        
        if cached_result:
            logger.debug(f"ğŸ” Vector search cache HIT: {query[:50]}...")
            
            # SearchResult ê°ì²´ë¡œ ë³€í™˜
            return SearchResult(
                query=cached_result["query"],
                documents=cached_result["documents"],
                embeddings=cached_result.get("embeddings", []),
                similarity_scores=cached_result["similarity_scores"],
                search_params=cached_result["search_params"],
                cached=True
            )
        
        logger.debug(f"âŒ Vector search cache MISS: {query[:50]}...")
        return None
    
    async def set_vector_search_result(self, search_result: SearchResult, 
                                     source: str = "mail", ttl: int = 1800) -> str:
        """ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ìºì‹œ ì €ì¥"""
        cache_key = self._generate_search_cache_key(
            search_result.query, 
            search_result.search_params, 
            source
        )
        
        cache_entry = {
            "query": search_result.query,
            "documents": search_result.documents,
            "embeddings": search_result.embeddings,
            "similarity_scores": search_result.similarity_scores,
            "search_params": search_result.search_params,
            "source": source,
            "cached_at": datetime.now().timestamp(),
            "result_count": len(search_result.documents)
        }
        
        cached_key = await self.cache_manager.set(
            RAGCacheNamespaces.VECTOR_SEARCH,
            cache_key,
            cache_entry,
            version_data={"query": search_result.query, "params": search_result.search_params},
            strategy=CacheVersion.CONTENT_HASH,
            ttl=ttl
        )
        
        logger.debug(f"ğŸ’¾ Vector search result cached: {search_result.query[:50]}...")
        return cached_key
    
    # ============================================================================
    # LLM ì‘ë‹µ ìºì‹œ ê´€ë¦¬
    # ============================================================================
    
    async def get_llm_response(self, prompt: str, model_name: str, 
                              parameters: Dict[str, Any]) -> Optional[LLMResponse]:
        """LLM ì‘ë‹µ ìºì‹œ ì¡°íšŒ"""
        cache_key = self._generate_llm_cache_key(prompt, model_name, parameters)
        
        cached_response = await self.cache_manager.get(
            RAGCacheNamespaces.LLM_RESPONSES,
            cache_key,
            version_data={"prompt": prompt, "model": model_name, "params": parameters},
            strategy=CacheVersion.CONTENT_HASH
        )
        
        if cached_response:
            logger.debug(f"ğŸ¤– LLM response cache HIT: {prompt[:50]}...")
            
            return LLMResponse(
                prompt=cached_response["prompt"],
                response=cached_response["response"],
                model_name=cached_response["model_name"],
                parameters=cached_response["parameters"],
                tokens_used=cached_response.get("tokens_used", 0),
                cached=True
            )
        
        logger.debug(f"âŒ LLM response cache MISS: {prompt[:50]}...")
        return None
    
    async def set_llm_response(self, llm_response: LLMResponse, ttl: int = 7200) -> str:
        """LLM ì‘ë‹µ ìºì‹œ ì €ì¥"""
        cache_key = self._generate_llm_cache_key(
            llm_response.prompt,
            llm_response.model_name,
            llm_response.parameters
        )
        
        cache_entry = {
            "prompt": llm_response.prompt,
            "response": llm_response.response,
            "model_name": llm_response.model_name,
            "parameters": llm_response.parameters,
            "tokens_used": llm_response.tokens_used,
            "cached_at": datetime.now().timestamp(),
            "response_length": len(llm_response.response)
        }
        
        cached_key = await self.cache_manager.set(
            RAGCacheNamespaces.LLM_RESPONSES,
            cache_key,
            cache_entry,
            version_data={
                "prompt": llm_response.prompt,
                "model": llm_response.model_name,
                "params": llm_response.parameters
            },
            strategy=CacheVersion.CONTENT_HASH,
            ttl=ttl
        )
        
        logger.debug(f"ğŸ’¾ LLM response cached: {llm_response.prompt[:50]}...")
        return cached_key
    
    # ============================================================================
    # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìºì‹œ ê´€ë¦¬
    # ============================================================================
    
    async def get_document_metadata(self, document_id: str, source: str) -> Optional[Dict[str, Any]]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìºì‹œ ì¡°íšŒ"""
        cache_key = f"{source}:{document_id}"
        
        return await self.cache_manager.get(
            RAGCacheNamespaces.DOCUMENT_METADATA,
            cache_key,
            version_data={"doc_id": document_id, "source": source},
            strategy=CacheVersion.INCREMENTAL
        )
    
    async def set_document_metadata(self, document_id: str, source: str, 
                                   metadata: Dict[str, Any], ttl: int = 86400) -> str:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìºì‹œ ì €ì¥"""
        cache_key = f"{source}:{document_id}"
        
        return await self.cache_manager.set(
            RAGCacheNamespaces.DOCUMENT_METADATA,
            cache_key,
            metadata,
            version_data={"doc_id": document_id, "source": source},
            strategy=CacheVersion.INCREMENTAL,
            ttl=ttl
        )
    
    # ============================================================================
    # ì‚¬ìš©ì ì„¸ì…˜ ìºì‹œ ê´€ë¦¬
    # ============================================================================
    
    async def get_user_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """ì‚¬ìš©ì ì„¸ì…˜ ìºì‹œ ì¡°íšŒ"""
        return await self.cache_manager.get(
            RAGCacheNamespaces.USER_SESSIONS,
            session_id,
            version_data={"session_id": session_id},
            strategy=CacheVersion.TIMESTAMP
        )
    
    async def set_user_session(self, session_id: str, session_data: Dict[str, Any], 
                              ttl: int = 3600) -> str:
        """ì‚¬ìš©ì ì„¸ì…˜ ìºì‹œ ì €ì¥"""
        return await self.cache_manager.set(
            RAGCacheNamespaces.USER_SESSIONS,
            session_id,
            session_data,
            version_data={"session_id": session_id},
            strategy=CacheVersion.TIMESTAMP,
            ttl=ttl
        )
    
    # ============================================================================
    # ìºì‹œ ë¬´íš¨í™” ê´€ë¦¬
    # ============================================================================
    
    async def invalidate_embeddings(self, model_name: str = None) -> int:
        """ì„ë² ë”© ëª¨ë¸ ë³€ê²½ ì‹œ ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”"""
        if model_name:
            # íŠ¹ì • ëª¨ë¸ì˜ ì„ë² ë”©ë§Œ ë¬´íš¨í™” - íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ ì§ì ‘ ì²˜ë¦¬
            deleted_count = 0
            
            # ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ íŒ¨í„´ ë§¤ì¹­ ì‚­ì œ
            if self.cache_manager.memory_cache:
                pattern = f"{RAGCacheNamespaces.EMBEDDINGS}:.*:{model_name}#v#.*$"
                deleted_count += self.cache_manager.memory_cache.delete_by_pattern(pattern)
            
            # Redis ìºì‹œì—ì„œ íŒ¨í„´ ë§¤ì¹­ ì‚­ì œ
            if self.cache_manager.redis_cache:
                pattern = f"{RAGCacheNamespaces.EMBEDDINGS}:*:{model_name}*"
                deleted_count += await self.cache_manager.redis_cache.delete_by_pattern(pattern)
            
            logger.info(f"Invalidated embeddings for model {model_name}: {deleted_count} entries")
            return deleted_count
        else:
            # ëª¨ë“  ì„ë² ë”© ìºì‹œ ë¬´íš¨í™”
            return await self.cache_manager.invalidate_namespace(RAGCacheNamespaces.EMBEDDINGS)
    
    async def invalidate_vector_search(self, source: str = None) -> int:
        """ë²¡í„° ê²€ìƒ‰ ìºì‹œ ë¬´íš¨í™”"""
        if source:
            # íŠ¹ì • ì†ŒìŠ¤ë§Œ ë¬´íš¨í™” (ë³µì¡í•œ íŒ¨í„´ì´ë¯€ë¡œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì „ì²´)
            return await self.cache_manager.invalidate_namespace(RAGCacheNamespaces.VECTOR_SEARCH)
        else:
            return await self.cache_manager.invalidate_namespace(RAGCacheNamespaces.VECTOR_SEARCH)
    
    async def invalidate_llm_responses(self, model_name: str = None) -> int:
        """LLM ì‘ë‹µ ìºì‹œ ë¬´íš¨í™”"""
        if model_name:
            # íŠ¹ì • ëª¨ë¸ì˜ ì‘ë‹µë§Œ ë¬´íš¨í™” (ë³µì¡í•œ êµ¬í˜„ í•„ìš”)
            return await self.cache_manager.invalidate_namespace(RAGCacheNamespaces.LLM_RESPONSES)
        else:
            return await self.cache_manager.invalidate_namespace(RAGCacheNamespaces.LLM_RESPONSES)
    
    async def invalidate_user_sessions(self, older_than_hours: int = 24) -> int:
        """ì˜¤ë˜ëœ ì‚¬ìš©ì ì„¸ì…˜ ë¬´íš¨í™”"""
        # ì‹œê°„ ê¸°ë°˜ ë¬´íš¨í™”ëŠ” ìë™ TTLì— ì˜ì¡´
        return await self.cache_manager.invalidate_namespace(RAGCacheNamespaces.USER_SESSIONS)
    
    async def invalidate_all_rag_cache(self) -> Dict[str, int]:
        """ëª¨ë“  RAG ìºì‹œ ë¬´íš¨í™”"""
        results = {}
        
        for namespace in [
            RAGCacheNamespaces.EMBEDDINGS,
            RAGCacheNamespaces.VECTOR_SEARCH,
            RAGCacheNamespaces.LLM_RESPONSES,
            RAGCacheNamespaces.DOCUMENT_METADATA,
            RAGCacheNamespaces.USER_SESSIONS
        ]:
            count = await self.cache_manager.invalidate_namespace(namespace)
            results[namespace] = count
        
        logger.info(f"ğŸ§¹ All RAG cache invalidated: {results}")
        return results
    
    # ============================================================================
    # í†µê³„ ë° ëª¨ë‹ˆí„°ë§
    # ============================================================================
    
    async def get_rag_cache_stats(self) -> Dict[str, Any]:
        """RAG ìºì‹œ í†µê³„"""
        base_stats = self.cache_manager.get_stats()
        
        # RAG íŠ¹í™” í†µê³„ ì¶”ê°€
        rag_stats = {
            "rag_cache_version": "1.0.0",
            "embedding_model_version": self.embedding_model_version,
            "namespaces": [
                RAGCacheNamespaces.EMBEDDINGS,
                RAGCacheNamespaces.VECTOR_SEARCH,
                RAGCacheNamespaces.LLM_RESPONSES,
                RAGCacheNamespaces.DOCUMENT_METADATA,
                RAGCacheNamespaces.USER_SESSIONS
            ]
        }
        
        return {**base_stats, "rag_specific": rag_stats}
    
    # ============================================================================
    # ë‚´ë¶€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
    # ============================================================================
    
    def _normalize_query(self, query: str) -> str:
        """ì¿¼ë¦¬ ì •ê·œí™”"""
        # ê³µë°± ì •ë¦¬, ì†Œë¬¸ì ë³€í™˜, íŠ¹ìˆ˜ë¬¸ì ì œê±°
        normalized = query.strip().lower()
        normalized = ' '.join(normalized.split())  # ì—°ì† ê³µë°± ì œê±°
        return normalized
    
    def _generate_search_cache_key(self, query: str, search_params: Dict[str, Any], 
                                  source: str) -> str:
        """ë²¡í„° ê²€ìƒ‰ ìºì‹œ í‚¤ ìƒì„±"""
        normalized_query = self._normalize_query(query)
        
        # ê²€ìƒ‰ íŒŒë¼ë¯¸í„°ë¥¼ ì •ë ¬ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
        sorted_params = json.dumps(search_params, sort_keys=True)
        
        key_components = [normalized_query, sorted_params, source]
        combined_key = "|".join(key_components)
        
        # í‚¤ê°€ ë„ˆë¬´ ê¸¸ë©´ í•´ì‹œ ì‚¬ìš©
        if len(combined_key) > 200:
            return hashlib.md5(combined_key.encode()).hexdigest()
        
        return combined_key.replace(" ", "_").replace(":", "_")
    
    def _generate_llm_cache_key(self, prompt: str, model_name: str, 
                               parameters: Dict[str, Any]) -> str:
        """LLM ìºì‹œ í‚¤ ìƒì„±"""
        # í”„ë¡¬í”„íŠ¸ í•´ì‹œ (ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìŒ)
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()[:16]
        
        # íŒŒë¼ë¯¸í„° í•´ì‹œ
        params_str = json.dumps(parameters, sort_keys=True)
        params_hash = hashlib.md5(params_str.encode()).hexdigest()[:8]
        
        return f"{model_name}:{prompt_hash}:{params_hash}"

# ì „ì—­ RAG ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_rag_cache_manager: Optional[RAGCacheManager] = None

async def get_rag_cache_manager() -> RAGCacheManager:
    """ê¸€ë¡œë²Œ RAG ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _rag_cache_manager
    
    if _rag_cache_manager is None:
        _rag_cache_manager = RAGCacheManager()
        await _rag_cache_manager.initialize()
    
    return _rag_cache_manager