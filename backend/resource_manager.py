"""
Resource Manager Module for HDí˜„ëŒ€ë¯¸í¬ Gauss-1 RAG System
Author: Claude Code
Date: 2025-01-26
Description: í˜„ì‹¤ì ì¸ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ - Ollama ë™ì‹œì„± í† í° ë²„í‚· + Qdrant í´ë¼ì´ì–¸íŠ¸ í’€ë§
"""

import os
import asyncio
import logging
import time
import json
from typing import Dict, Optional, Any, List, Union, AsyncIterator
from dataclasses import dataclass, field
from collections import deque
from contextlib import asynccontextmanager
import httpx
import threading
from datetime import datetime, timedelta
from enum import Enum

logger = logging.getLogger(__name__)

class CircuitBreakerState(Enum):
    CLOSED = "closed"      # ì •ìƒ ë™ì‘
    OPEN = "open"          # ì¥ì•  ìƒíƒœ
    HALF_OPEN = "half_open"  # ë³µêµ¬ ì‹œë„

@dataclass
class ResourceConfig:
    """ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ì„¤ì •"""
    # Ollama ë™ì‹œì„± ì œì–´
    ollama_max_concurrency: int = int(os.getenv("OLLAMA_MAX_CONCURRENCY", "8"))
    ollama_endpoint: str = os.getenv("OLLAMA_ENDPOINT", "http://127.0.0.1:11434")
    
    # ì„ë² ë”© ì„¤ì • ì¶”ê°€
    embed_backend: str = os.getenv("EMBED_BACKEND", "st")
    embed_model: str = os.getenv("EMBED_MODEL", "BAAI/bge-m3") 
    embed_device: str = os.getenv("EMBED_DEVICE", "auto")
    embed_batch: int = int(os.getenv("EMBED_BATCH", "64"))
    
    # Collection ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì • ì¶”ê°€
    qdrant_namespace: str = os.getenv("QDRANT_NAMESPACE", "default")
    qdrant_env: str = os.getenv("QDRANT_ENV", "dev")
    
    def get_collection_name(self, source_type: str, base_name: str = "documents") -> str:
        """ë™ì  ì»¬ë ‰ì…˜ëª… ìƒì„±"""
        return f"{self.qdrant_namespace}_{self.qdrant_env}_{source_type}_{base_name}"
    
    # íŒŒì´í”„ë¼ì¸ ì„¤ì •
    app_max_concurrency: int = int(os.getenv("APP_MAX_CONCURRENCY", "8"))
    queue_max: int = int(os.getenv("QUEUE_MAX", "64"))
    
    # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    http_timeout_ms: int = int(os.getenv("HTTP_TIMEOUT_MS", "3000"))
    http_max_keepalive: int = int(os.getenv("HTTP_MAX_KEEPALIVE", "20"))
    
    # ì¬ì‹œë„ ì •ì±…
    retry_max: int = int(os.getenv("RETRY_MAX", "3"))
    retry_backoff_ms: int = int(os.getenv("RETRY_BACKOFF_MS", "200"))
    
    # Circuit Breaker ì„¤ì • (í˜„ì‹¤ì  ê¸°ì¤€)
    cb_error_threshold: float = float(os.getenv("CB_ERROR_THRESHOLD", "0.20"))  # 20% ì—ëŸ¬ìœ¨
    cb_p95_threshold_ms: int = int(os.getenv("CB_P95_THRESHOLD_MS", "5000"))   # P95 5ì´ˆ
    cb_queue_threshold: int = int(os.getenv("CB_QUEUE_THRESHOLD", "64"))       # í ê¸¸ì´ 64
    cb_window_seconds: int = int(os.getenv("CB_WINDOW_SECONDS", "30"))         # 30ì´ˆ ìœˆë„ìš°
    cb_recovery_seconds: int = int(os.getenv("CB_RECOVERY_SECONDS", "10"))     # 10ì´ˆ í›„ ë°˜ê°œë°©
    
    # í ê´€ë¦¬
    queue_max_size: int = int(os.getenv("QUEUE_MAX_SIZE", "64"))

@dataclass
class RequestMetrics:
    """ìš”ì²­ ë©”íŠ¸ë¦­ ë°ì´í„°"""
    timestamp: float
    duration_ms: float
    success: bool
    error: Optional[str] = None

class CircuitBreaker:
    """
    í˜„ì‹¤ì ì¸ Circuit Breaker - ì—ëŸ¬ìœ¨Â·P95Â·í ê¸¸ì´ ê¸°ë°˜
    CPU/RAM ê¸°ì¤€ì´ ì•„ë‹Œ ìš´ì˜ ì§€í‘œ ê¸°ë°˜ìœ¼ë¡œ ì¬ì„¤ê³„
    """
    
    def __init__(self, name: str, config: ResourceConfig):
        self.name = name
        self.config = config
        self.state = CircuitBreakerState.CLOSED
        self.failure_count = 0
        self.last_failure_time = 0
        self.metrics: deque[RequestMetrics] = deque(maxlen=1000)  # ìµœê·¼ 1000ê°œ ìš”ì²­
        self.lock = threading.Lock()
        
        logger.info(f"ğŸ”§ Circuit Breaker '{name}' initialized with realistic thresholds")
        
    def _clean_old_metrics(self):
        """ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ ì •ë¦¬"""
        cutoff = time.time() - self.config.cb_window_seconds
        while self.metrics and self.metrics[0].timestamp < cutoff:
            self.metrics.popleft()
    
    def _calculate_error_rate(self) -> float:
        """ìµœê·¼ ìœˆë„ìš° ë‚´ ì—ëŸ¬ìœ¨ ê³„ì‚°"""
        if not self.metrics:
            return 0.0
        
        total = len(self.metrics)
        errors = sum(1 for m in self.metrics if not m.success)
        return errors / total if total > 0 else 0.0
    
    def _calculate_p95_latency(self) -> float:
        """P95 ì§€ì—°ì‹œê°„ ê³„ì‚°"""
        if not self.metrics:
            return 0.0
        
        durations = sorted([m.duration_ms for m in self.metrics])
        p95_index = int(len(durations) * 0.95)
        return durations[p95_index] if durations else 0.0
    
    def should_allow_request(self, current_queue_size: int = 0) -> bool:
        """ìš”ì²­ í—ˆìš© ì—¬ë¶€ ê²°ì •"""
        with self.lock:
            self._clean_old_metrics()
            
            if self.state == CircuitBreakerState.OPEN:
                # ë³µêµ¬ ì‹œê°„ í™•ì¸
                if time.time() - self.last_failure_time > self.config.cb_recovery_seconds:
                    self.state = CircuitBreakerState.HALF_OPEN
                    logger.info(f"ğŸ”„ Circuit Breaker '{self.name}' entering HALF_OPEN state")
                    return True
                return False
            
            if self.state == CircuitBreakerState.HALF_OPEN:
                # ë°˜ê°œë°© ìƒíƒœì—ì„œëŠ” ìƒ˜í”Œ ìš”ì²­ë§Œ í—ˆìš©
                return True
            
            # CLOSED ìƒíƒœ: í˜„ì‹¤ì  ì§€í‘œ ê¸°ë°˜ íŒë‹¨
            error_rate = self._calculate_error_rate()
            p95_latency = self._calculate_p95_latency()
            
            # ê°œë°© ì¡°ê±´ í™•ì¸
            should_open = (
                error_rate >= self.config.cb_error_threshold or
                p95_latency >= self.config.cb_p95_threshold_ms or
                current_queue_size >= self.config.cb_queue_threshold
            )
            
            if should_open:
                self.state = CircuitBreakerState.OPEN
                self.last_failure_time = time.time()
                logger.warning(
                    f"ğŸš¨ Circuit Breaker '{self.name}' OPENED: "
                    f"error_rate={error_rate:.2%}, p95={p95_latency:.0f}ms, queue={current_queue_size}"
                )
                return False
            
            return True
    
    def record_success(self, duration_ms: float):
        """ì„±ê³µ ê¸°ë¡"""
        with self.lock:
            self.metrics.append(RequestMetrics(
                timestamp=time.time(),
                duration_ms=duration_ms,
                success=True
            ))
            
            if self.state == CircuitBreakerState.HALF_OPEN:
                # ë°˜ê°œë°©ì—ì„œ ì—°ì† ì„±ê³µ ì‹œ ë‹«ê¸°
                recent_successes = sum(1 for m in list(self.metrics)[-5:] if m.success)
                if recent_successes >= 5:
                    self.state = CircuitBreakerState.CLOSED
                    self.failure_count = 0
                    logger.info(f"âœ… Circuit Breaker '{self.name}' CLOSED (recovered)")
    
    def record_failure(self, duration_ms: float, error: str):
        """ì‹¤íŒ¨ ê¸°ë¡"""
        with self.lock:
            self.metrics.append(RequestMetrics(
                timestamp=time.time(),
                duration_ms=duration_ms,
                success=False,
                error=error[:100]  # ì—ëŸ¬ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ
            ))
            
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.state == CircuitBreakerState.HALF_OPEN:
                # ë°˜ê°œë°©ì—ì„œ ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ ê°œë°©
                self.state = CircuitBreakerState.OPEN
                logger.warning(f"ğŸ”´ Circuit Breaker '{self.name}' back to OPEN (half-open failed)")
    
    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        with self.lock:
            self._clean_old_metrics()
            return {
                "name": self.name,
                "state": self.state.value,
                "error_rate": self._calculate_error_rate(),
                "p95_latency_ms": self._calculate_p95_latency(),
                "recent_requests": len(self.metrics),
                "failure_count": self.failure_count,
                "last_failure": datetime.fromtimestamp(self.last_failure_time).isoformat() if self.last_failure_time else None
            }

class OllamaTokenBucket:
    """
    Ollama ë™ì‹œì„± í† í° ë²„í‚· ê´€ë¦¬
    - íŒŒì´ì¬ ê°ì²´ í’€ë§ì´ ì•„ë‹Œ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ + ì•± ë ˆë²¨ ë™ì‹œì„± ì œí•œ
    """
    
    def __init__(self, config: ResourceConfig):
        self.config = config
        self.semaphore = asyncio.Semaphore(config.ollama_max_concurrency)
        self.active_requests = 0
        self.queue_size = 0
        self.lock = asyncio.Lock()
        self.circuit_breaker = CircuitBreaker("ollama", config)
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ (ì¬ì‚¬ìš©)
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(config.http_timeout_ms / 1000.0),
            limits=httpx.Limits(
                max_keepalive_connections=config.http_max_keepalive,
                max_connections=config.http_max_keepalive * 2,
                keepalive_expiry=30.0
            )
        )
        
        logger.info(f"ğŸ¯ Ollama Token Bucket initialized: concurrency={config.ollama_max_concurrency}")
    
    @asynccontextmanager
    async def acquire_token(self):
        """í† í° íšë“ ë° ë°˜í™˜"""
        async with self.lock:
            self.queue_size += 1
        
        # Circuit Breaker í™•ì¸
        if not self.circuit_breaker.should_allow_request(self.queue_size):
            async with self.lock:
                self.queue_size -= 1
            raise Exception(f"Circuit breaker OPEN for Ollama service")
        
        try:
            await self.semaphore.acquire()
            async with self.lock:
                self.queue_size -= 1
                self.active_requests += 1
            
            yield
            
        finally:
            async with self.lock:
                self.active_requests -= 1
            self.semaphore.release()
    
    async def generate_with_retry(self, prompt: str, model: str = "gemma3:4b") -> str:
        """ì¬ì‹œë„ ì •ì±…ì´ í¬í•¨ëœ LLM ìƒì„±"""
        for attempt in range(self.config.retry_max):
            start_time = time.time()
            
            try:
                async with self.acquire_token():
                    response = await self._call_ollama_api(prompt, model)
                    
                duration_ms = (time.time() - start_time) * 1000
                self.circuit_breaker.record_success(duration_ms)
                return response
                
            except Exception as e:
                duration_ms = (time.time() - start_time) * 1000
                self.circuit_breaker.record_failure(duration_ms, str(e))
                
                if attempt < self.config.retry_max - 1:
                    # ì§€ìˆ˜ ë°±ì˜¤í”„
                    delay = self.config.retry_backoff_ms * (2 ** attempt) / 1000.0
                    logger.warning(f"ğŸ”„ Ollama retry {attempt + 1}/{self.config.retry_max} after {delay:.1f}s: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"âŒ Ollama failed after {self.config.retry_max} attempts: {e}")
                    raise
    
    async def _call_ollama_api(self, prompt: str, model: str) -> str:
        """ì‹¤ì œ Ollama API í˜¸ì¶œ"""
        try:
            response = await self.client.post(
                f"{self.config.ollama_endpoint}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                }
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "")
            
        except httpx.TimeoutException:
            raise Exception("Ollama request timeout")
        except httpx.HTTPError as e:
            raise Exception(f"Ollama HTTP error: {e}")
        except Exception as e:
            raise Exception(f"Ollama API error: {e}")
    
    async def get_status(self) -> Dict[str, Any]:
        """í† í° ë²„í‚· ìƒíƒœ ë°˜í™˜"""
        async with self.lock:
            return {
                "max_concurrency": self.config.ollama_max_concurrency,
                "active_requests": self.active_requests,
                "queue_size": self.queue_size,
                "available_tokens": self.semaphore._value,
                "circuit_breaker": self.circuit_breaker.get_status()
            }
    
    async def generate_embedding(self, model: str, text: str) -> List[float]:
        """í†µì¼ëœ ì„ë² ë”© ìƒì„± ë©”ì„œë“œ - ENV ê¸°ë°˜ ì—”ë“œí¬ì¸íŠ¸ + input í•„ë“œ"""
        async with self.acquire_token():
            try:
                # ENV ê¸°ë°˜ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (í•˜ë“œì½”ë”© ì œê±°) - ìŠ¬ë˜ì‹œ ì¤‘ë³µ ë°©ì§€
                base = (self.config.ollama_endpoint or "").rstrip("/")
                endpoint = f"{base}/api/embeddings"
                
                response = await self.client.post(
                    endpoint,
                    json={
                        "model": model,
                        "input": text  # prompt â†’ input í•„ë“œë¡œ ìˆ˜ì • (Ollama API ê·œê²©)
                    },
                    timeout=30.0
                )
                response.raise_for_status()
                result = response.json()
                return result.get("embedding", [])
            except Exception as e:
                logger.error(f"âŒ Ollama embedding failed: {e}")
                raise

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.client.aclose()

class QdrantClientPool:
    """
    Qdrant í´ë¼ì´ì–¸íŠ¸ í’€ - httpx Keep-Alive ê¸°ë°˜
    ì»¤ë„¥ì…˜ ìƒí•œ + íƒ€ì„ì•„ì›ƒ/ì¬ì‹œë„ ê³µí†µí™”
    """
    
    def __init__(self, source_type: str, config: ResourceConfig):
        self.source_type = source_type
        self.config = config
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ URL ë¡œë“œ
        if source_type == "mail":
            self.base_url = os.getenv("MAIL_QDRANT_URL", "http://127.0.0.1:6333")
        elif source_type == "doc":
            self.base_url = os.getenv("DOC_QDRANT_URL", "http://10.0.0.12:6333")
        else:
            raise ValueError(f"Unknown source type: {source_type}")
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            timeout=httpx.Timeout(config.http_timeout_ms / 1000.0),
            limits=httpx.Limits(
                max_keepalive_connections=config.http_max_keepalive,
                max_connections=config.http_max_keepalive * 2,
                keepalive_expiry=30.0
            )
        )
        
        self.circuit_breaker = CircuitBreaker(f"qdrant_{source_type}", config)
        
        logger.info(f"ğŸ”— Qdrant Client Pool [{source_type}] â†’ {self.base_url}")
    
    async def search_with_retry(self, collection_name: str, query_vector: List[float], 
                               limit: int = 3, score_threshold: float = 0.3) -> List[Dict]:
        """ì¬ì‹œë„ ì •ì±…ì´ í¬í•¨ëœ ë²¡í„° ê²€ìƒ‰"""
        for attempt in range(self.config.retry_max):
            start_time = time.time()
            
            try:
                # Circuit Breaker í™•ì¸
                if not self.circuit_breaker.should_allow_request():
                    raise Exception(f"Circuit breaker OPEN for Qdrant {self.source_type}")
                
                response = await self._search_qdrant(collection_name, query_vector, limit, score_threshold)
                
                duration_ms = (time.time() - start_time) * 1000
                self.circuit_breaker.record_success(duration_ms)
                return response
                
            except Exception as e:
                duration_ms = (time.time() - start_time) * 1000
                self.circuit_breaker.record_failure(duration_ms, str(e))
                
                if attempt < self.config.retry_max - 1:
                    delay = self.config.retry_backoff_ms * (2 ** attempt) / 1000.0
                    logger.warning(f"ğŸ”„ Qdrant {self.source_type} retry {attempt + 1}/{self.config.retry_max} after {delay:.1f}s: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"âŒ Qdrant {self.source_type} failed after {self.config.retry_max} attempts: {e}")
                    raise
    
    async def _search_qdrant(self, collection_name: str, query_vector: List[float], 
                           limit: int, score_threshold: float) -> List[Dict]:
        """ì‹¤ì œ Qdrant API í˜¸ì¶œ"""
        try:
            response = await self.client.post(
                f"/collections/{collection_name}/points/search",
                json={
                    "vector": query_vector,
                    "limit": limit,
                    "score_threshold": score_threshold,
                    "with_payload": True,
                    "with_vector": False
                }
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get("result", [])
            
        except httpx.TimeoutException:
            raise Exception(f"Qdrant {self.source_type} timeout")
        except httpx.HTTPError as e:
            raise Exception(f"Qdrant {self.source_type} HTTP error: {e}")
        except Exception as e:
            raise Exception(f"Qdrant {self.source_type} API error: {e}")
    
    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ì²´í¬"""
        try:
            start_time = time.time()
            response = await self.client.get("/")
            duration_ms = (time.time() - start_time) * 1000
            
            return {
                "source_type": self.source_type,
                "url": self.base_url,
                "healthy": response.status_code == 200,
                "response_time_ms": duration_ms,
                "circuit_breaker": self.circuit_breaker.get_status()
            }
        except Exception as e:
            return {
                "source_type": self.source_type,
                "url": self.base_url,
                "healthy": False,
                "error": str(e),
                "circuit_breaker": self.circuit_breaker.get_status()
            }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.client.aclose()

class ResourceManager:
    """
    í†µí•© ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ì - í˜„ì‹¤ì  ì„¤ê³„
    - Ollama: ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ + í† í° ë²„í‚·
    - Qdrant: HTTP í´ë¼ì´ì–¸íŠ¸ í’€ë§
    - Circuit Breaker: ìš´ì˜ ì§€í‘œ ê¸°ë°˜
    """
    
    def __init__(self, config: Optional[ResourceConfig] = None):
        self.config = config or ResourceConfig()
        
        # ì„ë² ë”© ê´€ë ¨ ì†ì„±
        self.embedder = None
        self.embed_backend = config.embed_backend
        self.embed_model = config.embed_model
        self.embed_device = None  # Will be resolved in from_env()
        
        # Ollama í† í° ë²„í‚·
        self.ollama_bucket = OllamaTokenBucket(self.config)
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ í’€
        self.qdrant_pools: Dict[str, QdrantClientPool] = {}
        for source_type in ["mail", "doc"]:
            try:
                self.qdrant_pools[source_type] = QdrantClientPool(source_type, self.config)
            except Exception as e:
                logger.error(f"âŒ Failed to initialize Qdrant pool for {source_type}: {e}")
        
        logger.info(f"ğŸ›ï¸ ResourceManager initialized with realistic design")
        logger.info(f"   ğŸ“Š Ollama concurrency: {self.config.ollama_max_concurrency}")
        logger.info(f"   ğŸ”— Qdrant pools: {list(self.qdrant_pools.keys())}")
        logger.info(f"   â±ï¸ Timeouts: {self.config.http_timeout_ms}ms")
        logger.info(f"   ğŸ”„ Retries: {self.config.retry_max} with {self.config.retry_backoff_ms}ms backoff")
    
    async def generate_llm_response(self, prompt: str, model: str = "gemma3:4b") -> str:
        """LLM ì‘ë‹µ ìƒì„± (í† í° ë²„í‚· + Circuit Breaker)"""
        return await self.ollama_bucket.generate_with_retry(prompt, model)
    
    async def search_vectors(self, source_type: str, collection_name: str, 
                           query_vector: List[float], limit: int = 3, 
                           score_threshold: float = 0.3) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ (í’€ë§ + Circuit Breaker)"""
        if source_type not in self.qdrant_pools:
            raise ValueError(f"Unknown source type: {source_type}")
        
        pool = self.qdrant_pools[source_type]
        return await pool.search_with_retry(collection_name, query_vector, limit, score_threshold)
    
    async def get_system_status(self) -> Dict[str, Any]:
        """ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        status = {
            "timestamp": datetime.now().isoformat(),
            "resource_manager": "v2.0.0-realistic",
            "ollama": await self.ollama_bucket.get_status(),
            "qdrant_pools": {}
        }
        
        for source_type, pool in self.qdrant_pools.items():
            status["qdrant_pools"][source_type] = await pool.health_check()
        
        return status
    
    def _resolve_device(self, spec: str) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if spec == "auto":
            try:
                import torch
                if torch.cuda.is_available():
                    logger.info(f"ğŸš€ GPU detected: {torch.cuda.get_device_name(0)}")
                    return "cuda:0"
            except Exception as e:
                logger.warning(f"âš ï¸ GPU check failed: {e}")
            logger.info("ğŸ“± Using CPU device")
            return "cpu"
        return spec
    
    def _load_st_model(self, model_name: str, device: str) -> Any:
        """SentenceTransformer ëª¨ë¸ ë¡œë“œ"""
        try:
            from sentence_transformers import SentenceTransformer
            import torch
            
            model = SentenceTransformer(model_name, device=device)
            
            # GPUì—ì„œ ì»´íŒŒì¼ ìµœì í™” ì‹œë„
            if device.startswith("cuda") and hasattr(torch, 'compile'):
                try:
                    model = torch.compile(model)
                    logger.info("ğŸ”¥ Model compilation enabled (PyTorch 2.0+)")
                except Exception as e:
                    logger.warning(f"âš ï¸ Model compilation failed: {e}")
            
            logger.info(f"âœ… Loaded {model_name} on {device}")
            return model
        except Exception as e:
            logger.error(f"âŒ Failed to load model {model_name}: {e}")
            raise
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ìƒì„±"""
        if not self.embedder:
            raise ValueError("Embedder not initialized")
        
        try:
            # ë°°ì¹˜ í¬ê¸° ì œí•œ
            batch_size = min(self.config.embed_batch, len(texts))
            
            if self.embed_backend == "st":
                # SentenceTransformer ì‚¬ìš©
                embeddings = await asyncio.to_thread(
                    self.embedder.encode,
                    texts,
                    batch_size=batch_size,
                    show_progress_bar=False,
                    convert_to_tensor=False,
                    normalize_embeddings=True
                )
                return embeddings.tolist() if hasattr(embeddings, 'tolist') else embeddings
            
            elif self.embed_backend == "ollama":
                # í†µì¼ëœ í´ë¼ì´ì–¸íŠ¸ ë˜í¼ ì‚¬ìš©
                embeddings = []
                for text in texts:
                    embedding = await self.ollama_bucket.generate_embedding(
                        model=self.embed_model,
                        text=text
                    )
                    embeddings.append(embedding)
                return embeddings
            
            else:
                raise ValueError(f"Unknown embedding backend: {self.embed_backend}")
                
        except Exception as e:
            logger.error(f"âŒ Embedding generation failed: {e}")
            raise
    
    @classmethod
    def from_env(cls) -> 'ResourceManager':
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ ë° ì´ˆê¸°í™”"""
        config = ResourceConfig()
        manager = cls(config)
        
        # ë””ë°”ì´ìŠ¤ í•´ê²°
        manager.embed_device = manager._resolve_device(config.embed_device)
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        if config.embed_backend == "st":
            manager.embedder = manager._load_st_model(config.embed_model, manager.embed_device)
        
        logger.info(f"ğŸ¯ ResourceManager initialized with {config.embed_backend} on {manager.embed_device}")
        return manager
    
    async def search_vectors(
        self, 
        source_type: str, 
        collection_name: str,
        query_vector: List[float], 
        limit: int = 10,
        score_threshold: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """ë²¡í„° ê²€ìƒ‰ í”„ë¡ì‹œ ë©”ì„œë“œ"""
        try:
            client = self.clients.get_qdrant_client(source_type)
            
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            results = await asyncio.to_thread(
                client.search,
                collection_name=collection_name,
                query_vector=query_vector,
                limit=limit,
                score_threshold=score_threshold,
                with_payload=True,
                with_vectors=False  # ì„±ëŠ¥ ìµœì í™”
            )
            
            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
            formatted_results = []
            for result in results:
                formatted_results.append({
                    "id": result.id,
                    "score": result.score,
                    "payload": result.payload
                })
            
            logger.info(f"ğŸ” Vector search completed: {len(formatted_results)} results")
            return formatted_results
            
        except Exception as e:
            logger.error(f"âŒ Vector search failed: {e}")
            raise
    
    async def generate_llm_response(self, prompt: str, model: str, stream: bool = False):
        """Unified LLM response generation with streaming support
        
        Args:
            prompt: The prompt to send to LLM
            model: Model name (e.g., 'gemma3:4b') 
            stream: If True, returns AsyncIterator[str], else returns str
            
        Returns:
            For stream=False: Complete response as string
            For stream=True: AsyncIterator yielding response tokens
        """
        base = (self.config.ollama_endpoint or "").rstrip("/")
        endpoint = f"{base}/api/generate"
        
        if not stream:
            # Non-streaming path
            resp = await self.ollama_bucket.client.post(
                endpoint, 
                json={"model": model, "prompt": prompt, "stream": False}, 
                timeout=120.0
            )
            resp.raise_for_status()
            data = resp.json()
            return data.get("response", "")

        # Streaming path
        async def _gen() -> AsyncIterator[str]:
            async with self.ollama_bucket.client.stream(
                "POST", 
                endpoint, 
                json={"model": model, "prompt": prompt, "stream": True}, 
                timeout=120.0
            ) as r:
                r.raise_for_status()
                async for line in r.aiter_lines():
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        yield obj.get("response", "")
                    except Exception:
                        # ë°©ì–´ì  íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¼ì¸ ê·¸ëŒ€ë¡œ í˜ë ¤ë³´ëƒ„
                        yield ""
        return _gen()

    async def cleanup(self):
        """ì „ì²´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ ResourceManager cleanup started")
        
        # Ollama ì •ë¦¬
        await self.ollama_bucket.cleanup()
        
        # Qdrant í’€ ì •ë¦¬
        for pool in self.qdrant_pools.values():
            await pool.cleanup()
        
        logger.info("âœ… ResourceManager cleanup completed")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_resource_manager: Optional[ResourceManager] = None

async def get_resource_manager() -> ResourceManager:
    """ì „ì—­ ë¦¬ì†ŒìŠ¤ ë§¤ë‹ˆì € ë°˜í™˜"""
    global _resource_manager
    if _resource_manager is None:
        _resource_manager = ResourceManager()
    return _resource_manager

async def cleanup_resources():
    """ì „ì—­ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    global _resource_manager
    if _resource_manager:
        await _resource_manager.cleanup()
        _resource_manager = None