"""
Resource Manager Module for HDí˜„ëŒ€ë¯¸í¬ Gauss-1 RAG System
Author: Claude Code
Date: 2025-01-26
Description: í˜„ì‹¤ì ì¸ ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ - Ollama ë™ì‹œì„± í† í° ë²„í‚· + Qdrant í´ë¼ì´ì–¸íŠ¸ í’€ë§
"""

import os
import asyncio
import logging
import time
import json
from typing import Dict, Optional, Any, List, Union, AsyncIterator
from dataclasses import dataclass, field
from collections import deque
from contextlib import asynccontextmanager
import httpx
import threading
from datetime import datetime, timedelta
from enum import Enum

logger = logging.getLogger(__name__)

# P1-4: Import metrics (if available)
try:
    from backend.common.logging import (
        QDRANT_ERR, SEARCH_LAT, EMBED_LAT, CACHE_HITS, CACHE_MISSES
    )
    METRICS_ENABLED = True
except ImportError:
    logger.warning("Metrics not available, running without metrics collection")
    METRICS_ENABLED = False

class CircuitBreakerState(Enum):
    CLOSED = "closed"      # ì •ìƒ ë™ì‘
    OPEN = "open"          # ì¥ì•  ìƒíƒœ
    HALF_OPEN = "half_open"  # ë³µêµ¬ ì‹œë„

@dataclass
class ResourceConfig:
    """ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ì„¤ì •"""
    # Ollama ë™ì‹œì„± ì œì–´
    ollama_max_concurrency: int = int(os.getenv("OLLAMA_MAX_CONCURRENCY", "8"))
    ollama_endpoint: str = os.getenv("OLLAMA_ENDPOINT", "http://127.0.0.1:11434")
    
    # ì„ë² ë”© ì„¤ì • ì¶”ê°€
    embed_backend: str = os.getenv("EMBED_BACKEND", "st")
    embed_model: str = os.getenv("EMBED_MODEL", "BAAI/bge-m3") 
    embed_device: str = os.getenv("EMBED_DEVICE", "auto")
    embed_batch: int = int(os.getenv("EMBED_BATCH", "64"))
    
    # Collection ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì„¤ì • ì¶”ê°€
    qdrant_namespace: str = os.getenv("QDRANT_NAMESPACE", "default")
    qdrant_env: str = os.getenv("QDRANT_ENV", "dev")
    
    def get_collection_name(self, source_type: str, base_name: str = "documents") -> str:
        """ë™ì  ì»¬ë ‰ì…˜ëª… ìƒì„±"""
        return f"{self.qdrant_namespace}_{self.qdrant_env}_{source_type}_{base_name}"
    
    # íŒŒì´í”„ë¼ì¸ ì„¤ì •
    app_max_concurrency: int = int(os.getenv("APP_MAX_CONCURRENCY", "8"))
    queue_max: int = int(os.getenv("QUEUE_MAX", "64"))
    
    # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    http_timeout_ms: int = int(os.getenv("HTTP_TIMEOUT_MS", "3000"))
    http_max_keepalive: int = int(os.getenv("HTTP_MAX_KEEPALIVE", "20"))
    
    # ì¬ì‹œë„ ì •ì±…
    retry_max: int = int(os.getenv("RETRY_MAX", "3"))
    retry_backoff_ms: int = int(os.getenv("RETRY_BACKOFF_MS", "200"))
    
    # Circuit Breaker ì„¤ì • (í˜„ì‹¤ì  ê¸°ì¤€)
    cb_error_threshold: float = float(os.getenv("CB_ERROR_THRESHOLD", "0.20"))  # 20% ì—ëŸ¬ìœ¨
    cb_p95_threshold_ms: int = int(os.getenv("CB_P95_THRESHOLD_MS", "5000"))   # P95 5ì´ˆ
    cb_queue_threshold: int = int(os.getenv("CB_QUEUE_THRESHOLD", "64"))       # í ê¸¸ì´ 64
    cb_window_seconds: int = int(os.getenv("CB_WINDOW_SECONDS", "30"))         # 30ì´ˆ ìœˆë„ìš°
    cb_recovery_seconds: int = int(os.getenv("CB_RECOVERY_SECONDS", "10"))     # 10ì´ˆ í›„ ë°˜ê°œë°©
    
    # í ê´€ë¦¬
    queue_max_size: int = int(os.getenv("QUEUE_MAX_SIZE", "64"))

@dataclass
class RequestMetrics:
    """ìš”ì²­ ë©”íŠ¸ë¦­ ë°ì´í„°"""
    timestamp: float
    duration_ms: float
    success: bool
    error: Optional[str] = None

class CircuitBreaker:
    """
    í˜„ì‹¤ì ì¸ Circuit Breaker - ì—ëŸ¬ìœ¨Â·P95Â·í ê¸¸ì´ ê¸°ë°˜
    CPU/RAM ê¸°ì¤€ì´ ì•„ë‹Œ ìš´ì˜ ì§€í‘œ ê¸°ë°˜ìœ¼ë¡œ ì¬ì„¤ê³„
    """
    
    def __init__(self, name: str, config: ResourceConfig):
        self.name = name
        self.config = config
        self.state = CircuitBreakerState.CLOSED
        self.failure_count = 0
        self.last_failure_time = 0
        self.metrics: deque[RequestMetrics] = deque(maxlen=1000)  # ìµœê·¼ 1000ê°œ ìš”ì²­
        self.lock = threading.Lock()
        
        logger.info(f"ğŸ”§ Circuit Breaker '{name}' initialized with realistic thresholds")
        
    def _clean_old_metrics(self):
        """ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ ì •ë¦¬"""
        cutoff = time.time() - self.config.cb_window_seconds
        while self.metrics and self.metrics[0].timestamp < cutoff:
            self.metrics.popleft()
    
    def _calculate_error_rate(self) -> float:
        """ìµœê·¼ ìœˆë„ìš° ë‚´ ì—ëŸ¬ìœ¨ ê³„ì‚°"""
        if not self.metrics:
            return 0.0
        
        total = len(self.metrics)
        errors = sum(1 for m in self.metrics if not m.success)
        return errors / total if total > 0 else 0.0
    
    def _calculate_p95_latency(self) -> float:
        """P95 ì§€ì—°ì‹œê°„ ê³„ì‚°"""
        if not self.metrics:
            return 0.0
        
        durations = sorted([m.duration_ms for m in self.metrics])
        p95_index = int(len(durations) * 0.95)
        return durations[p95_index] if durations else 0.0
    
    def should_allow_request(self, current_queue_size: int = 0) -> bool:
        """ìš”ì²­ í—ˆìš© ì—¬ë¶€ ê²°ì •"""
        with self.lock:
            self._clean_old_metrics()
            
            if self.state == CircuitBreakerState.OPEN:
                # ë³µêµ¬ ì‹œê°„ í™•ì¸
                if time.time() - self.last_failure_time > self.config.cb_recovery_seconds:
                    self.state = CircuitBreakerState.HALF_OPEN
                    logger.info(f"ğŸ”„ Circuit Breaker '{self.name}' entering HALF_OPEN state")
                    return True
                return False
            
            if self.state == CircuitBreakerState.HALF_OPEN:
                # ë°˜ê°œë°© ìƒíƒœì—ì„œëŠ” ìƒ˜í”Œ ìš”ì²­ë§Œ í—ˆìš©
                return True
            
            # CLOSED ìƒíƒœ: í˜„ì‹¤ì  ì§€í‘œ ê¸°ë°˜ íŒë‹¨
            error_rate = self._calculate_error_rate()
            p95_latency = self._calculate_p95_latency()
            
            # ê°œë°© ì¡°ê±´ í™•ì¸
            should_open = (
                error_rate >= self.config.cb_error_threshold or
                p95_latency >= self.config.cb_p95_threshold_ms or
                current_queue_size >= self.config.cb_queue_threshold
            )
            
            if should_open:
                self.state = CircuitBreakerState.OPEN
                self.last_failure_time = time.time()
                logger.warning(
                    f"ğŸš¨ Circuit Breaker '{self.name}' OPENED: "
                    f"error_rate={error_rate:.2%}, p95={p95_latency:.0f}ms, queue={current_queue_size}"
                )
                return False
            
            return True
    
    def record_success(self, duration_ms: float):
        """ì„±ê³µ ê¸°ë¡"""
        with self.lock:
            self.metrics.append(RequestMetrics(
                timestamp=time.time(),
                duration_ms=duration_ms,
                success=True
            ))
            
            if self.state == CircuitBreakerState.HALF_OPEN:
                # ë°˜ê°œë°©ì—ì„œ ì—°ì† ì„±ê³µ ì‹œ ë‹«ê¸°
                recent_successes = sum(1 for m in list(self.metrics)[-5:] if m.success)
                if recent_successes >= 5:
                    self.state = CircuitBreakerState.CLOSED
                    self.failure_count = 0
                    logger.info(f"âœ… Circuit Breaker '{self.name}' CLOSED (recovered)")
    
    def record_failure(self, duration_ms: float, error: str):
        """ì‹¤íŒ¨ ê¸°ë¡"""
        with self.lock:
            self.metrics.append(RequestMetrics(
                timestamp=time.time(),
                duration_ms=duration_ms,
                success=False,
                error=error[:100]  # ì—ëŸ¬ ë©”ì‹œì§€ ê¸¸ì´ ì œí•œ
            ))
            
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.state == CircuitBreakerState.HALF_OPEN:
                # ë°˜ê°œë°©ì—ì„œ ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ ê°œë°©
                self.state = CircuitBreakerState.OPEN
                logger.warning(f"ğŸ”´ Circuit Breaker '{self.name}' back to OPEN (half-open failed)")
    
    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ ë°˜í™˜"""
        with self.lock:
            self._clean_old_metrics()
            return {
                "name": self.name,
                "state": self.state.value,
                "error_rate": self._calculate_error_rate(),
                "p95_latency_ms": self._calculate_p95_latency(),
                "recent_requests": len(self.metrics),
                "failure_count": self.failure_count,
                "last_failure": datetime.fromtimestamp(self.last_failure_time).isoformat() if self.last_failure_time else None
            }

class OllamaTokenBucket:
    """
    Ollama ë™ì‹œì„± í† í° ë²„í‚· ê´€ë¦¬
    - íŒŒì´ì¬ ê°ì²´ í’€ë§ì´ ì•„ë‹Œ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ + ì•± ë ˆë²¨ ë™ì‹œì„± ì œí•œ
    """
    
    def __init__(self, config: ResourceConfig):
        self.config = config
        self.semaphore = asyncio.Semaphore(config.ollama_max_concurrency)
        self.active_requests = 0
        self.queue_size = 0
        self.lock = asyncio.Lock()
        self.circuit_breaker = CircuitBreaker("ollama", config)
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ (ì¬ì‚¬ìš©)
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(config.http_timeout_ms / 1000.0),
            limits=httpx.Limits(
                max_keepalive_connections=config.http_max_keepalive,
                max_connections=config.http_max_keepalive * 2,
                keepalive_expiry=30.0
            )
        )
        
        logger.info(f"ğŸ¯ Ollama Token Bucket initialized: concurrency={config.ollama_max_concurrency}")
    
    @asynccontextmanager
    async def acquire_token(self):
        """í† í° íšë“ ë° ë°˜í™˜"""
        async with self.lock:
            self.queue_size += 1
        
        # Circuit Breaker í™•ì¸
        if not self.circuit_breaker.should_allow_request(self.queue_size):
            async with self.lock:
                self.queue_size -= 1
            raise Exception(f"Circuit breaker OPEN for Ollama service")
        
        try:
            await self.semaphore.acquire()
            async with self.lock:
                self.queue_size -= 1
                self.active_requests += 1
            
            yield
            
        finally:
            async with self.lock:
                self.active_requests -= 1
            self.semaphore.release()
    
    async def generate_with_retry(self, prompt: str, model: str = "gemma3:4b") -> str:
        """ì¬ì‹œë„ ì •ì±…ì´ í¬í•¨ëœ LLM ìƒì„±"""
        for attempt in range(self.config.retry_max):
            start_time = time.time()
            
            try:
                async with self.acquire_token():
                    response = await self._call_ollama_api(prompt, model)
                    
                duration_ms = (time.time() - start_time) * 1000
                self.circuit_breaker.record_success(duration_ms)
                return response
                
            except Exception as e:
                duration_ms = (time.time() - start_time) * 1000
                self.circuit_breaker.record_failure(duration_ms, str(e))
                
                if attempt < self.config.retry_max - 1:
                    # ì§€ìˆ˜ ë°±ì˜¤í”„
                    delay = self.config.retry_backoff_ms * (2 ** attempt) / 1000.0
                    logger.warning(f"ğŸ”„ Ollama retry {attempt + 1}/{self.config.retry_max} after {delay:.1f}s: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"âŒ Ollama failed after {self.config.retry_max} attempts: {e}")
                    raise
    
    async def _call_ollama_api(self, prompt: str, model: str) -> str:
        """ì‹¤ì œ Ollama API í˜¸ì¶œ"""
        try:
            response = await self.client.post(
                f"{self.config.ollama_endpoint}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                }
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "")
            
        except httpx.TimeoutException:
            raise Exception("Ollama request timeout")
        except httpx.HTTPError as e:
            raise Exception(f"Ollama HTTP error: {e}")
        except Exception as e:
            raise Exception(f"Ollama API error: {e}")
    
    async def get_status(self) -> Dict[str, Any]:
        """í† í° ë²„í‚· ìƒíƒœ ë°˜í™˜"""
        async with self.lock:
            return {
                "max_concurrency": self.config.ollama_max_concurrency,
                "active_requests": self.active_requests,
                "queue_size": self.queue_size,
                "available_tokens": self.semaphore._value,
                "circuit_breaker": self.circuit_breaker.get_status()
            }
    
    async def generate_embedding(self, model: str, text: str) -> List[float]:
        """í†µì¼ëœ ì„ë² ë”© ìƒì„± ë©”ì„œë“œ - ENV ê¸°ë°˜ ì—”ë“œí¬ì¸íŠ¸ + input í•„ë“œ"""
        async with self.acquire_token():
            try:
                # ENV ê¸°ë°˜ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (í•˜ë“œì½”ë”© ì œê±°) - ìŠ¬ë˜ì‹œ ì¤‘ë³µ ë°©ì§€
                base = (self.config.ollama_endpoint or "").rstrip("/")
                endpoint = f"{base}/api/embeddings"
                
                response = await self.client.post(
                    endpoint,
                    json={
                        "model": model,
                        "input": text  # prompt â†’ input í•„ë“œë¡œ ìˆ˜ì • (Ollama API ê·œê²©)
                    },
                    timeout=30.0
                )
                response.raise_for_status()
                result = response.json()
                return result.get("embedding", [])
            except Exception as e:
                logger.error(f"âŒ Ollama embedding failed: {e}")
                raise

    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.client.aclose()

class QdrantClientPool:
    """
    Qdrant í´ë¼ì´ì–¸íŠ¸ í’€ - httpx Keep-Alive ê¸°ë°˜
    ì»¤ë„¥ì…˜ ìƒí•œ + íƒ€ì„ì•„ì›ƒ/ì¬ì‹œë„ ê³µí†µí™”
    """
    
    def __init__(self, source_type: str, config: ResourceConfig):
        self.source_type = source_type
        self.config = config
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ URL ë¡œë“œ
        if source_type == "mail":
            self.base_url = os.getenv("MAIL_QDRANT_URL", "http://127.0.0.1:6333")
        elif source_type == "doc":
            self.base_url = os.getenv("DOC_QDRANT_URL", "http://10.0.0.12:6333")
        else:
            raise ValueError(f"Unknown source type: {source_type}")
        
        # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            timeout=httpx.Timeout(config.http_timeout_ms / 1000.0),
            limits=httpx.Limits(
                max_keepalive_connections=config.http_max_keepalive,
                max_connections=config.http_max_keepalive * 2,
                keepalive_expiry=30.0
            )
        )
        
        self.circuit_breaker = CircuitBreaker(f"qdrant_{source_type}", config)
        
        logger.info(f"ğŸ”— Qdrant Client Pool [{source_type}] â†’ {self.base_url}")
    
    async def search_with_retry(self, collection_name: str, query_vector: List[float], 
                               limit: int = 3, score_threshold: float = 0.3) -> List[Dict]:
        """ì¬ì‹œë„ ì •ì±…ì´ í¬í•¨ëœ ë²¡í„° ê²€ìƒ‰"""
        for attempt in range(self.config.retry_max):
            start_time = time.time()
            
            try:
                # Circuit Breaker í™•ì¸
                if not self.circuit_breaker.should_allow_request():
                    raise Exception(f"Circuit breaker OPEN for Qdrant {self.source_type}")
                
                response = await self._search_qdrant(collection_name, query_vector, limit, score_threshold)
                
                duration_ms = (time.time() - start_time) * 1000
                self.circuit_breaker.record_success(duration_ms)
                return response
                
            except Exception as e:
                duration_ms = (time.time() - start_time) * 1000
                self.circuit_breaker.record_failure(duration_ms, str(e))
                
                if attempt < self.config.retry_max - 1:
                    delay = self.config.retry_backoff_ms * (2 ** attempt) / 1000.0
                    logger.warning(f"ğŸ”„ Qdrant {self.source_type} retry {attempt + 1}/{self.config.retry_max} after {delay:.1f}s: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"âŒ Qdrant {self.source_type} failed after {self.config.retry_max} attempts: {e}")
                    raise
    
    async def _search_qdrant(self, collection_name: str, query_vector: List[float], 
                           limit: int, score_threshold: float) -> List[Dict]:
        """ì‹¤ì œ Qdrant API í˜¸ì¶œ"""
        try:
            response = await self.client.post(
                f"/collections/{collection_name}/points/search",
                json={
                    "vector": query_vector,
                    "limit": limit,
                    "score_threshold": score_threshold,
                    "with_payload": True,
                    "with_vector": False
                }
            )
            response.raise_for_status()
            
            result = response.json()
            return result.get("result", [])
            
        except httpx.TimeoutException:
            raise Exception(f"Qdrant {self.source_type} timeout")
        except httpx.HTTPError as e:
            raise Exception(f"Qdrant {self.source_type} HTTP error: {e}")
        except Exception as e:
            raise Exception(f"Qdrant {self.source_type} API error: {e}")
    
    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ì²´í¬"""
        try:
            start_time = time.time()
            response = await self.client.get("/")
            duration_ms = (time.time() - start_time) * 1000
            
            return {
                "source_type": self.source_type,
                "url": self.base_url,
                "healthy": response.status_code == 200,
                "response_time_ms": duration_ms,
                "circuit_breaker": self.circuit_breaker.get_status()
            }
        except Exception as e:
            return {
                "source_type": self.source_type,
                "url": self.base_url,
                "healthy": False,
                "error": str(e),
                "circuit_breaker": self.circuit_breaker.get_status()
            }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.client.aclose()

class ResourceManager:
    """
    í†µí•© ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ì - í˜„ì‹¤ì  ì„¤ê³„
    - Ollama: ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ + í† í° ë²„í‚·
    - Qdrant: HTTP í´ë¼ì´ì–¸íŠ¸ í’€ë§
    - Circuit Breaker: ìš´ì˜ ì§€í‘œ ê¸°ë°˜
    """
    
    def __init__(self, config: Optional[ResourceConfig] = None):
        self.config = config or ResourceConfig()
        
        # ì„ë² ë”© ê´€ë ¨ ì†ì„±
        self.embedder = None
        self.embed_backend = config.embed_backend
        self.embed_model = config.embed_model
        self.embed_device = None  # Will be resolved in from_env()
        
        # Ollama í† í° ë²„í‚·
        self.ollama_bucket = OllamaTokenBucket(self.config)
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ í’€
        self.qdrant_pools: Dict[str, QdrantClientPool] = {}
        for source_type in ["mail", "doc"]:
            try:
                self.qdrant_pools[source_type] = QdrantClientPool(source_type, self.config)
            except Exception as e:
                logger.error(f"âŒ Failed to initialize Qdrant pool for {source_type}: {e}")
        
        logger.info(f"ğŸ›ï¸ ResourceManager initialized with realistic design")
        logger.info(f"   ğŸ“Š Ollama concurrency: {self.config.ollama_max_concurrency}")
        logger.info(f"   ğŸ”— Qdrant pools: {list(self.qdrant_pools.keys())}")
        logger.info(f"   â±ï¸ Timeouts: {self.config.http_timeout_ms}ms")
        logger.info(f"   ğŸ”„ Retries: {self.config.retry_max} with {self.config.retry_backoff_ms}ms backoff")
    
    async def generate_llm_response(self, prompt: str, model: str = "gemma3:4b") -> str:
        """LLM ì‘ë‹µ ìƒì„± (í† í° ë²„í‚· + Circuit Breaker)"""
        return await self.ollama_bucket.generate_with_retry(prompt, model)
    
    
    async def get_system_status(self) -> Dict[str, Any]:
        """ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        status = {
            "timestamp": datetime.now().isoformat(),
            "resource_manager": "v2.0.0-realistic",
            "ollama": await self.ollama_bucket.get_status(),
            "qdrant_pools": {}
        }
        
        for source_type, pool in self.qdrant_pools.items():
            status["qdrant_pools"][source_type] = await pool.health_check()
        
        return status
    
    def _resolve_device(self, spec: str) -> str:
        """ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€"""
        if spec == "auto":
            try:
                import torch
                if torch.cuda.is_available():
                    logger.info(f"ğŸš€ GPU detected: {torch.cuda.get_device_name(0)}")
                    return "cuda:0"
            except Exception as e:
                logger.warning(f"âš ï¸ GPU check failed: {e}")
            logger.info("ğŸ“± Using CPU device")
            return "cpu"
        return spec
    
    def _load_st_model(self, model_name: str, device: str) -> Any:
        """SentenceTransformer ëª¨ë¸ ë¡œë“œ"""
        try:
            from sentence_transformers import SentenceTransformer
            import torch
            
            model = SentenceTransformer(model_name, device=device)
            
            # GPUì—ì„œ ì»´íŒŒì¼ ìµœì í™” ì‹œë„
            if device.startswith("cuda") and hasattr(torch, 'compile'):
                try:
                    model = torch.compile(model)
                    logger.info("ğŸ”¥ Model compilation enabled (PyTorch 2.0+)")
                except Exception as e:
                    logger.warning(f"âš ï¸ Model compilation failed: {e}")
            
            logger.info(f"âœ… Loaded {model_name} on {device}")
            return model
        except Exception as e:
            logger.error(f"âŒ Failed to load model {model_name}: {e}")
            raise
    
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ìƒì„±"""
        # FIX #3 Support: Allow Ollama backend even without embedder
        if not self.embedder and self.embed_backend != "ollama":
            raise ValueError("Embedder not initialized for non-Ollama backend")
        
        try:
            # ë°°ì¹˜ í¬ê¸° ì œí•œ
            batch_size = min(self.config.embed_batch, len(texts))
            
            if self.embed_backend == "st":
                # SentenceTransformer ì‚¬ìš©
                embeddings = await asyncio.to_thread(
                    self.embedder.encode,
                    texts,
                    batch_size=batch_size,
                    show_progress_bar=False,
                    convert_to_tensor=False,
                    normalize_embeddings=True
                )
                return embeddings.tolist() if hasattr(embeddings, 'tolist') else embeddings
            
            elif self.embed_backend == "ollama":
                # í†µì¼ëœ í´ë¼ì´ì–¸íŠ¸ ë˜í¼ ì‚¬ìš©
                embeddings = []
                for text in texts:
                    embedding = await self.ollama_bucket.generate_embedding(
                        model=self.embed_model,
                        text=text
                    )
                    embeddings.append(embedding)
                return embeddings
            
            else:
                raise ValueError(f"Unknown embedding backend: {self.embed_backend}")
                
        except Exception as e:
            logger.error(f"âŒ Embedding generation failed: {e}")
            raise
    
    @classmethod
    def from_env(cls) -> 'ResourceManager':
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ ë° ì´ˆê¸°í™”"""
        config = ResourceConfig()
        manager = cls(config)
        
        # ë””ë°”ì´ìŠ¤ í•´ê²°
        manager.embed_device = manager._resolve_device(config.embed_device)
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        if config.embed_backend == "st":
            manager.embedder = manager._load_st_model(config.embed_model, manager.embed_device)
        
        logger.info(f"ğŸ¯ ResourceManager initialized with {config.embed_backend} on {manager.embed_device}")
        return manager
    
    async def search_vectors(
        self, 
        source_type: str,
        query_vector: List[float], 
        limit: int = 10,
        score_threshold: Optional[float] = None,
        with_payload: bool = True,
        with_vectors: bool = False,
        **kwargs
    ) -> List[Dict[str, Any]]:
        """í†µí•© ë²¡í„° ê²€ìƒ‰ í”„ë¡ì‹œ (íŒŒë¼ë¯¸í„° ì¤‘ë³µ ë°©ì§€)"""
        # Import scope context for dual routing
        from backend.common.qdrant_router import scope_ctx
        
        # P1-4: Start search timing
        if METRICS_ENABLED:
            search_start = time.perf_counter()
        
        # Get current scope from context
        scope = scope_ctx.get("personal")
        
        try:
            # í´ë¼ì´ì–¸íŠ¸ íšë“ (ìš°ì„ ìˆœìœ„: qdrant_router â†’ clients â†’ qdrant_pools)
            if hasattr(self, 'qdrant_router') and self.qdrant_router:
                # Use QdrantRouter for dual routing
                client = self.qdrant_router.get_client(scope)
                collection_name = self.qdrant_router.get_namespace(scope, source_type)
                logger.info(f"ğŸ”€ Routing search to {scope}: {collection_name}")
            elif hasattr(self, "clients") and self.clients:
                client = self.clients.get_qdrant_client(source_type)
                collection_name = None  # Will be determined below
            else:
                client = self.qdrant_pools[source_type].client
                collection_name = None  # Will be determined below
            
            # SecureQdrantClient ê°ì§€ (ì´ë¯¸ collection_name ë‚´ì¥)
            if hasattr(client, 'collection_name'):
                # SecureQdrantClient: collection_name ì „ë‹¬ ê¸ˆì§€
                results = await asyncio.to_thread(
                    client.search,
                    query_vector=query_vector,  # keyword only
                    limit=limit,
                    score_threshold=score_threshold,
                    with_payload=with_payload,
                    with_vectors=with_vectors,
                    **kwargs
                )
            else:
                # ì¼ë°˜ QdrantClient: collection_name í•„ìš”
                collection_name = self.get_default_collection_name(source_type, "my_documents")
                
                if hasattr(client, 'search'):
                    # ë™ê¸° í´ë¼ì´ì–¸íŠ¸: positional ì¸ì ì‚¬ìš©
                    results = await asyncio.to_thread(
                        client.search,
                        collection_name,  # positional
                        query_vector,     # positional
                        limit=limit,
                        score_threshold=score_threshold,
                        with_payload=with_payload,
                        with_vectors=with_vectors,
                        **kwargs
                    )
                elif hasattr(client, 'search_async'):
                    # ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸: keyword ì¸ì ì‚¬ìš©
                    results = await client.search_async(
                        collection_name=collection_name,
                        query_vector=query_vector,
                        limit=limit,
                        score_threshold=score_threshold,
                        with_payload=with_payload,
                        with_vectors=with_vectors,
                        **kwargs
                    )
                else:
                    raise RuntimeError(f"Client {type(client)} does not support search methods")
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for result in results:
                formatted_results.append({
                    "id": getattr(result, 'id', None),
                    "score": getattr(result, 'score', 0.0),
                    "payload": getattr(result, 'payload', {})
                })
            
            # P1-4: Record search latency with scope
            if METRICS_ENABLED:
                # Add scope label if available
                if hasattr(self, 'qdrant_router') and self.qdrant_router:
                    SEARCH_LAT.labels(backend="qdrant", source=source_type, scope=scope).observe(time.perf_counter() - search_start)
                else:
                    SEARCH_LAT.labels(backend="qdrant", source=source_type).observe(time.perf_counter() - search_start)
            
            logger.info(f"ğŸ” Vector search completed: {len(formatted_results)} results for {source_type}")
            return formatted_results
            
        except Exception as e:
            # P1-4: Record search error
            if METRICS_ENABLED:
                QDRANT_ERR.labels(type="resource_manager_search").inc()
            logger.error(f"âŒ Vector search failed for {scope}: {e}")
            
            # Check if we should fallback from dept to personal
            if scope == "dept" and os.getenv("QDRANT_DEPT_FALLBACK") == "personal":
                logger.warning(f"ğŸ”„ Attempting fallback from dept to personal")
                
                # Set fallback flag in request state if available
                request = kwargs.get("request")
                if request and hasattr(request, "state"):
                    setattr(request.state, "fallback_used", True)
                
                # Switch scope to personal and retry
                scope_ctx.set("personal")
                try:
                    return await self.search_vectors(
                        source_type=source_type,
                        query_vector=query_vector,
                        limit=limit,
                        score_threshold=score_threshold,
                        with_payload=with_payload,
                        with_vectors=with_vectors,
                        **kwargs
                    )
                except Exception as fallback_error:
                    logger.error(f"âŒ Fallback to personal also failed: {fallback_error}")
                    raise fallback_error
            else:
                raise
    
    async def generate_llm_response(self, prompt: str, model: str, stream: bool = False):
        """Unified LLM response generation with streaming support
        
        Args:
            prompt: The prompt to send to LLM
            model: Model name (e.g., 'gemma3:4b') 
            stream: If True, returns AsyncIterator[str], else returns str
            
        Returns:
            For stream=False: Complete response as string
            For stream=True: AsyncIterator yielding response tokens
        """
        base = (self.config.ollama_endpoint or "").rstrip("/")
        endpoint = f"{base}/api/generate"
        
        if not stream:
            # Non-streaming path
            resp = await self.ollama_bucket.client.post(
                endpoint, 
                json={"model": model, "prompt": prompt, "stream": False}, 
                timeout=120.0
            )
            resp.raise_for_status()
            data = resp.json()
            return data.get("response", "")

        # Streaming path
        async def _gen() -> AsyncIterator[str]:
            async with self.ollama_bucket.client.stream(
                "POST", 
                endpoint, 
                json={"model": model, "prompt": prompt, "stream": True}, 
                timeout=120.0
            ) as r:
                r.raise_for_status()
                async for line in r.aiter_lines():
                    if not line:
                        continue
                    try:
                        obj = json.loads(line)
                        yield obj.get("response", "")
                    except Exception:
                        # ë°©ì–´ì  íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¼ì¸ ê·¸ëŒ€ë¡œ í˜ë ¤ë³´ëƒ„
                        yield ""
        return _gen()

    # ğŸ”§ Collection Management Helper Methods
    def get_default_collection_name(self, source_type: str, base_name: str = "my_documents") -> str:
        """
        ì»¬ë ‰ì…˜ëª… ìƒì„±ì„ ìœ„í•œ ë‹¨ì¼ ì ‘ê·¼ì 
        
        Args:
            source_type: 'mail' | 'doc' | 'attachment' ë“±
            base_name: ê¸°ë³¸ ì»¬ë ‰ì…˜ëª… (default: "my_documents")
            
        Returns:
            ì™„ì „í•œ ì»¬ë ‰ì…˜ëª… (ì˜ˆ: "hdmipo_dev_mail_my_documents")
            
        Example:
            >>> rm.get_default_collection_name("mail")
            "hdmipo_dev_mail_my_documents"
        """
        return self.config.get_collection_name(source_type, base_name)

    async def get_embedding_dim(self) -> int:
        """
        í˜„ì¬ ì„ë² ë”© ëª¨ë¸ì˜ ì°¨ì› ë™ì  ê°ì§€
        
        Returns:
            ì„ë² ë”© ë²¡í„° ì°¨ì› (ì˜ˆ: 1024 for BGE-M3)
            
        Raises:
            ValueError: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ê²°ê³¼
            RuntimeError: ì„ë² ë” ì´ˆê¸°í™” ì‹¤íŒ¨
        """
        # FIX #3: Remove embedder guard to support Ollama backend
        # The embed_texts() method handles backend differences internally
        try:
            # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ í…ìŠ¤íŠ¸ë¡œ ì°¨ì› ê°ì§€
            test_embeddings = await self.embed_texts(["__dim_check__"])
            
            if not test_embeddings or not test_embeddings[0]:
                raise ValueError("Empty embedding result from model")
                
            dimension = len(test_embeddings[0])
            
            if dimension <= 0:
                raise ValueError(f"Invalid embedding dimension: {dimension}")
                
            logger.info(f"ğŸ”¢ Detected embedding dimension: {dimension}")
            return dimension
            
        except Exception as e:
            logger.error(f"âŒ Failed to detect embedding dimension: {e}")
            # ìƒì„¸í•œ ì§„ë‹¨ ì •ë³´ ì œê³µ
            logger.error(f"ğŸ” Embedder info - Backend: {self.config.embed_backend}, "
                        f"Model: {self.config.embed_model}, Device: {self.config.embed_device}")
            raise

    async def startup_vector_dim_check(
        self,
        sources: Optional[List[str]] = None,
        base_name: str = "my_documents",
        auto_create: bool = False
    ) -> Dict[str, Any]:
        """
        ìŠ¤íƒ€íŠ¸ì—… ì‹œ ì»¬ë ‰ì…˜ ì¡´ì¬ ë° ë²¡í„° ì°¨ì› ì¼ì¹˜ì„± ê²€ì¦
        
        Args:
            sources: ê²€ì¦í•  ì†ŒìŠ¤ íƒ€ì… ëª©ë¡ (default: ["mail", "doc"])
            base_name: ê¸°ë³¸ ì»¬ë ‰ì…˜ëª… (default: "my_documents")
            auto_create: ì»¬ë ‰ì…˜ ìë™ ìƒì„± ì—¬ë¶€ (default: False, ìš´ì˜ í™˜ê²½ ê¶Œì¥)
            
        Returns:
            ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            
        Raises:
            RuntimeError: ì»¬ë ‰ì…˜ ë¶€ì¬ ë˜ëŠ” ì°¨ì› ë¶ˆì¼ì¹˜
            ConnectionError: Qdrant ì—°ê²° ì‹¤íŒ¨
        """
        sources = sources or ["mail", "doc"]
        expected_dim = await self.get_embedding_dim()
        
        # FIX #1: Add missing fields for main.py compatibility while keeping existing ones
        validation_results = {
            # Original fields (backward compatibility)
            "expected_dimension": expected_dim,
            "collections_checked": {},
            "errors": [],
            "warnings": [],
            
            # New fields required by main.py
            "overall_status": "pending",  # Will be set to 'ok' or 'error'
            "collection_status": {},  # Source-specific status
            "issues": [],  # List of issue descriptions
            "summary": "",  # Human-readable summary
            "embedding_dimension": expected_dim,  # main.py expects this exact name
            "validation_summary": {  # Structured summary
                "total_collections": len(sources),
                "successful_collections": 0,
                "failed_collections": 0,
                "warnings": 0
            }
        }
        
        logger.info(f"ğŸ” Starting collection validation for sources: {sources}")
        logger.info(f"ğŸ”¢ Expected embedding dimension: {expected_dim}")
        
        for source_type in sources:
            collection_name = self.get_default_collection_name(source_type, base_name)
            
            try:
                # FIX #2: Unify client access using get_qdrant_client pattern
                # Check if we have the clients attribute (from main.py)
                if hasattr(self, 'clients') and self.clients:
                    client = self.clients.get_qdrant_client(source_type)
                else:
                    # Fallback to qdrant_pools if clients not available
                    if source_type not in self.qdrant_pools:
                        raise ValueError(f"Unsupported source type: {source_type}")
                    client = self.qdrant_pools[source_type].client
                
                collection_info = await self._validate_single_collection(
                    client, collection_name, expected_dim, auto_create
                )
                
                # Update both old and new fields for compatibility
                validation_results["collections_checked"][source_type] = collection_info
                validation_results["collection_status"][source_type] = {
                    "status": "ok",
                    "collection_name": collection_name,
                    "dimension": collection_info.get("dimension", expected_dim),
                    "message": f"Collection '{collection_name}' validated successfully"
                }
                validation_results["validation_summary"]["successful_collections"] += 1
                logger.info(f"âœ… Collection '{collection_name}' validation passed")
                
            except Exception as e:
                error_msg = f"Collection '{collection_name}' validation failed: {e}"
                
                # Update both old and new error tracking fields
                validation_results["errors"].append(error_msg)
                validation_results["issues"].append(error_msg)
                validation_results["collection_status"][source_type] = {
                    "status": "error",
                    "collection_name": collection_name,
                    "error": str(e),
                    "message": error_msg
                }
                validation_results["validation_summary"]["failed_collections"] += 1
                
                logger.error(f"âŒ {error_msg}")
                
                # ìš´ì˜íŒ€ì„ ìœ„í•œ ìƒì„¸ ì§„ë‹¨ ì •ë³´
                logger.error(f"ğŸ”§ Troubleshooting for '{collection_name}':")
                logger.error(f"   - Expected dimension: {expected_dim}")
                logger.error(f"   - Source type: {source_type}")
                logger.error(f"   - Full collection name: {collection_name}")
                
                # ìë™ ìƒì„±ì´ ë¹„í™œì„±í™”ëœ ê²½ìš° ê°€ì´ë“œ ì œê³µ
                if not auto_create and "not found" in str(e).lower():
                    logger.info(f"ğŸ’¡ To create collection manually:")
                    logger.info(f"   python scripts/create_collections.py --source {source_type}")
        
        # Set overall status and summary based on results
        # CRITICAL FIX: Use "success"/"warning"/"error" to match main.py expectations
        if validation_results["errors"]:
            validation_results["overall_status"] = "error"
            validation_results["summary"] = f"Validation failed: {len(validation_results['errors'])} collection(s) with errors"
            
            error_summary = "; ".join(validation_results["errors"])
            
            # Don't raise error immediately - let main.py decide based on overall_status
            logger.warning(f"âš ï¸ Validation completed with errors: {error_summary}")
        elif validation_results["warnings"]:
            validation_results["overall_status"] = "warning"
            validation_results["summary"] = (
                f"All {validation_results['validation_summary']['successful_collections']} "
                f"collection(s) validated with {len(validation_results['warnings'])} warning(s)"
            )
            logger.info(f"âš ï¸ Collections validated with warnings: {list(validation_results['collections_checked'].keys())}")
        else:
            validation_results["overall_status"] = "success"  # Changed from "ok" to "success"
            validation_results["summary"] = (
                f"All {validation_results['validation_summary']['successful_collections']} "
                f"collection(s) validated successfully with dimension {expected_dim}"
            )
            logger.info(f"ğŸ‰ All collections validated successfully: {list(validation_results['collections_checked'].keys())}")
        
        # Add warnings count
        validation_results["validation_summary"]["warnings"] = len(validation_results.get("warnings", []))
        
        return validation_results

    async def _validate_single_collection(
        self, 
        client, 
        collection_name: str, 
        expected_dim: int,
        auto_create: bool = False
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ì»¬ë ‰ì…˜ ê²€ì¦ (ë‚´ë¶€ í—¬í¼)"""
        
        try:
            # 1ë‹¨ê³„: ì»¬ë ‰ì…˜ ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹œë„
            info = await asyncio.to_thread(client.get_collection, collection_name)
            
            # 2ë‹¨ê³„: ë²¡í„° ì°¨ì› ì¶”ì¶œ (ë‹¤ì–‘í•œ qdrant-client ë²„ì „ í˜¸í™˜)
            actual_dim = self._extract_vector_dimension(info)
            
            if actual_dim and actual_dim != expected_dim:
                raise RuntimeError(
                    f"Vector dimension mismatch - Expected: {expected_dim}, "
                    f"Actual: {actual_dim}. Collection may need recreation."
                )
            
            # 3ë‹¨ê³„: ì¶”ê°€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            collection_info = {
                "name": collection_name,
                "dimension": actual_dim or expected_dim,
                "status": "healthy",
                "points_count": getattr(info, 'points_count', 0),
                "config": {
                    "distance": str(getattr(getattr(info, 'config', None), 'distance', None)),
                    "hnsw_config": self._extract_hnsw_config(info)
                }
            }
            
            return collection_info
            
        except Exception as meta_error:
            # 4ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ë“œë¼ì´ëŸ° ê²€ìƒ‰ìœ¼ë¡œ ì¬ì‹œë„
            logger.debug(f"Metadata query failed for '{collection_name}', trying dry-run search")
            
            try:
                # ë”ë¯¸ ë²¡í„°ë¡œ ê²€ìƒ‰ ì‹œë„ (ì¡´ì¬ ë° ì°¨ì› í™•ì¸)
                # Check if client has search method (sync) or search_async (async)
                if hasattr(client, 'search'):
                    await asyncio.to_thread(
                        client.search,
                        collection_name=collection_name,
                        query_vector=[0.0] * expected_dim,
                        limit=1,
                        with_payload=False,
                        with_vectors=False,
                    )
                elif hasattr(client, 'search_async'):
                    await client.search_async(
                        collection_name=collection_name,
                        query_vector=[0.0] * expected_dim,
                        limit=1,
                        with_payload=False,
                        with_vectors=False,
                    )
                else:
                    # Skip search validation if method not available
                    logger.warning(f"Search method not available for client, skipping validation")
                
                # ê²€ìƒ‰ ì„±ê³µ ì‹œ ì°¨ì›ì€ ë§ë‹¤ê³  ê°€ì •
                return {
                    "name": collection_name,
                    "dimension": expected_dim,
                    "status": "accessible_via_search",
                    "note": "Metadata query failed but search succeeded"
                }
                
            except Exception as search_error:
                error_msg = str(search_error).lower()
                
                # 5ë‹¨ê³„: êµ¬ì²´ì ì¸ ì—ëŸ¬ íƒ€ì… ë¶„ë¥˜ ë° ì²˜ë¦¬
                if "not found" in error_msg or "does not exist" in error_msg:
                    if auto_create:
                        logger.info(f"ğŸ”¨ Auto-creating collection '{collection_name}' with dimension {expected_dim}")
                        await self._create_collection_with_defaults(client, collection_name, expected_dim)
                        return {
                            "name": collection_name,
                            "dimension": expected_dim,
                            "status": "auto_created"
                        }
                    else:
                        raise RuntimeError(
                            f"Collection '{collection_name}' not found (expected dim={expected_dim}). "
                            f"Create manually or enable auto_create=True."
                        )
                
                elif any(dim_keyword in error_msg for dim_keyword in 
                        ["vector size", "different size", "dimension", "mismatch"]):
                    raise RuntimeError(
                        f"Vector dimension mismatch for '{collection_name}' "
                        f"(expected {expected_dim}). Collection needs recreation."
                    )
                
                else:
                    # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „íŒŒ
                    raise RuntimeError(f"Unexpected error validating '{collection_name}': {search_error}")

    def _extract_vector_dimension(self, collection_info) -> Optional[int]:
        """ì»¬ë ‰ì…˜ ì •ë³´ì—ì„œ ë²¡í„° ì°¨ì› ì¶”ì¶œ (ë²„ì „ í˜¸í™˜)"""
        try:
            config = getattr(collection_info, "config", None)
            if not config:
                return None
                
            params = getattr(config, "params", None)
            if not params:
                return None
                
            vectors = getattr(params, "vectors", None)
            if not vectors:
                return None
            
            # Case 1: vectors.size (ë‹¨ì¼ ë²¡í„° ì„¤ì •)
            if hasattr(vectors, "size"):
                return vectors.size
            
            # Case 2: vectors dict (ë‹¤ì¤‘ ë²¡í„° ì„¤ì •)
            elif isinstance(vectors, dict) and vectors:
                first_vector_config = list(vectors.values())[0]
                return getattr(first_vector_config, "size", None)
            
            return None
            
        except Exception as e:
            logger.debug(f"Failed to extract vector dimension: {e}")
            return None

    def _extract_hnsw_config(self, collection_info) -> Dict[str, Any]:
        """HNSW ì„¤ì • ì¶”ì¶œ"""
        try:
            config = getattr(collection_info, "config", None)
            params = getattr(config, "params", None)
            hnsw = getattr(params, "hnsw_config", None)
            
            if hnsw:
                return {
                    "m": getattr(hnsw, "m", None),
                    "ef_construct": getattr(hnsw, "ef_construct", None),
                    "full_scan_threshold": getattr(hnsw, "full_scan_threshold", None)
                }
            return {}
        except:
            return {}

    async def _create_collection_with_defaults(
        self, 
        client, 
        collection_name: str, 
        dimension: int
    ):
        """ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì»¬ë ‰ì…˜ ìƒì„±"""
        try:
            from qdrant_client.models import VectorParams, Distance, HnswConfigDiff
            
            await asyncio.to_thread(
                client.create_collection,
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=dimension,
                    distance=Distance.COSINE,
                    hnsw_config=HnswConfigDiff(
                        m=16,
                        ef_construct=100,
                        full_scan_threshold=10000
                    )
                )
            )
            logger.info(f"âœ… Auto-created collection '{collection_name}' with dimension {dimension}")
        except Exception as e:
            logger.error(f"âŒ Failed to auto-create collection '{collection_name}': {e}")
            raise

    async def cleanup(self):
        """ì „ì²´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("ğŸ§¹ ResourceManager cleanup started")
        
        # Ollama ì •ë¦¬
        await self.ollama_bucket.cleanup()
        
        # Qdrant í’€ ì •ë¦¬
        for pool in self.qdrant_pools.values():
            await pool.cleanup()
        
        logger.info("âœ… ResourceManager cleanup completed")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_resource_manager: Optional[ResourceManager] = None

async def get_resource_manager() -> ResourceManager:
    """ì „ì—­ ë¦¬ì†ŒìŠ¤ ë§¤ë‹ˆì € ë°˜í™˜"""
    global _resource_manager
    if _resource_manager is None:
        _resource_manager = ResourceManager()
    return _resource_manager

async def cleanup_resources():
    """ì „ì—­ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
    global _resource_manager
    if _resource_manager:
        await _resource_manager.cleanup()
        _resource_manager = None